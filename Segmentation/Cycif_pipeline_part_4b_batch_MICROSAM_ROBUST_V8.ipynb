{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import json\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from scipy import ndimage as ndi\n",
    "from skimage import morphology\n",
    "from skimage.measure import label, regionprops, find_contours\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\u00e2\u0153\u201c Basic imports\")\n",
    "\n",
    "import torch\n",
    "from micro_sam.util import get_sam_model\n",
    "from micro_sam.instance_segmentation import (\n",
    "    AutomaticMaskGenerator,\n",
    "    mask_data_to_segmentation\n",
    ")\n",
    "print(f\"\u00e2\u0153\u201c PyTorch: {torch.__version__}\")\n",
    "print(f\"\u00e2\u0153\u201c Micro-SAM imported\")\n",
    "print(f\"\u00e2\u0153\u201c Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f102f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "DATA_ROOT = Path(r\"C:\\Users\\researcher\\data\\scimap-master\\data\")\n",
    "\n",
    "# Only the 4 remaining samples\n",
    "ALL_SAMPLES = [\n",
    "    \"sample_009\", \"sample_012\", \"sample_014\", \"sample_016\"\n",
    "]\n",
    "\n",
    "# Optional skip list\n",
    "SKIP_SAMPLES = []  # Set to [\"sample_012\"] to skip 13GB monster\n",
    "\n",
    "# Model\n",
    "MODEL_TYPE = 'vit_b'\n",
    "\n",
    "# ADAPTIVE TILE SIZING - PIXEL-BASED\n",
    "TILE_SIZE_NORMAL = 1024      # <60M pixels\n",
    "TILE_SIZE_MONSTER = 512      # 60-80M pixels\n",
    "TILE_SIZE_ULTRA = 384        # >80M pixels (prevents VS Code crash)\n",
    "OVERLAP = 256\n",
    "MIN_OBJECT_SIZE = 8000\n",
    "\n",
    "# Pixel count thresholds for adaptive tile sizing\n",
    "MONSTER_PIXEL_THRESHOLD = 60_000_000  # 60M pixels\n",
    "ULTRA_PIXEL_THRESHOLD = 80_000_000    # 80M pixels (sample_016 = 90.6M)\n",
    "\n",
    "# CHUNKED PROCESSING - ADAPTIVE BY IMAGE SIZE\n",
    "MASK_BATCH_SIZE = 10  # Base: 10 masks\n",
    "MASK_BATCH_SIZE_MONSTER = 6  # For huge images (60-80M px): 6 masks\n",
    "MASK_BATCH_SIZE_ULTRA = 4    # For ultra images (>80M px): 4 masks \u00e2\u2020\u2019 12 GB max\n",
    "TILE_LOAD_BATCH = 10  # Base: 10 tiles\n",
    "TILE_LOAD_BATCH_MONSTER = 5  # For huge images: 5 tiles\n",
    "TILE_LOAD_BATCH_ULTRA = 3    # For ultra images: 3 tiles \u00e2\u2020\u2019 10 GB max\n",
    "\n",
    "# Disk caching\n",
    "TEMP_DIR = Path(r\"C:\\Users\\researcher\\Downloads\\Cycif_pipeline_V3\\temp_tiles_v8\")\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use V5 checkpoint\n",
    "CHECKPOINT_FILE = Path(r\"C:\\Users\\researcher\\Downloads\\Cycif_pipeline_V3\\batch_progress_v5.json\")\n",
    "\n",
    "# Filtering\n",
    "MIN_AREA = 8000\n",
    "MAX_AREA = 100000\n",
    "MAX_ECCENTRICITY = 0.9\n",
    "\n",
    "# Morphology\n",
    "APPLY_MORPHOLOGY = True\n",
    "MERGE_RADIUS = 15\n",
    "SMOOTH_RADIUS = 5\n",
    "\n",
    "# RAM safety - STRICT\n",
    "MIN_FREE_RAM_GB = 8.0  # Don't start if <8GB free\n",
    "MIN_SAFE_RAM_GB = 5.0  # Pause processing if drops below 5GB\n",
    "PAUSE_BETWEEN_SAMPLES = 15  # Longer pause for GC\n",
    "\n",
    "print(f\"Data: {DATA_ROOT}\")\n",
    "print(f\"Samples: {len(ALL_SAMPLES)}\")\n",
    "if SKIP_SAMPLES:\n",
    "    print(f\"\u00e2\u0161\u00a0\u00ef\u00b8\u008f  Skipping: {SKIP_SAMPLES}\")\n",
    "print(f\"Adaptive tiles: {TILE_SIZE_NORMAL}px \u00e2\u2020\u2019 {TILE_SIZE_MONSTER}px \u00e2\u2020\u2019 {TILE_SIZE_ULTRA}px\")\n",
    "print(f\"Pixel thresholds: <60M \u00e2\u2020\u2019 60-80M \u00e2\u2020\u2019 >80M\")\n",
    "print(f\"Mask batch: {MASK_BATCH_SIZE} \u00e2\u2020\u2019 {MASK_BATCH_SIZE_MONSTER} \u00e2\u2020\u2019 {MASK_BATCH_SIZE_ULTRA}\")\n",
    "print(f\"Tile load: {TILE_LOAD_BATCH} \u00e2\u2020\u2019 {TILE_LOAD_BATCH_MONSTER} \u00e2\u2020\u2019 {TILE_LOAD_BATCH_ULTRA}\")\n",
    "print(f\"Temp: {TEMP_DIR}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_FILE}\")\n",
    "print(f\"RAM gate: {MIN_FREE_RAM_GB} GB minimum, {MIN_SAFE_RAM_GB} GB safe threshold\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64137b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECKPOINT & RAM MANAGEMENT ===\n",
    "def load_checkpoint():\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            success_count = len([r for r in data.get('results', []) if r.get('status') == 'success'])\n",
    "            print(f\"\u00e2\u0153\u201c Checkpoint: {success_count} successful\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"\u00e2\u0161\u00a0\u00ef\u00b8\u008f  Checkpoint error: {e}\")\n",
    "    return {'completed': [], 'failed': [], 'results': []}\n",
    "\n",
    "def save_checkpoint(data):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                print(f\"\u00e2\u0161\u00a0\u00ef\u00b8\u008f  Checkpoint save failed: {e}\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "def get_remaining_samples(checkpoint_data):\n",
    "    successful = set()\n",
    "    for result in checkpoint_data.get('results', []):\n",
    "        if result.get('status') == 'success':\n",
    "            successful.add(result['sample_id'])\n",
    "    return [s for s in ALL_SAMPLES if s not in successful and s not in SKIP_SAMPLES]\n",
    "\n",
    "def check_ram(min_required=MIN_FREE_RAM_GB, context=\"\"):\n",
    "    mem = psutil.virtual_memory()\n",
    "    free_gb = mem.available / (1024**3)\n",
    "    if free_gb < min_required:\n",
    "        print(f\"\u00e2\u0161\u00a0\u00ef\u00b8\u008f  LOW RAM: {free_gb:.1f} GB (need {min_required} GB) - {context}\")\n",
    "    return free_gb\n",
    "\n",
    "def wait_for_ram(min_required=MIN_SAFE_RAM_GB, max_wait=60):\n",
    "    \"\"\"Wait for RAM to free up.\"\"\"\n",
    "    start = time.time()\n",
    "    while time.time() - start < max_wait:\n",
    "        free_gb = psutil.virtual_memory().available / (1024**3)\n",
    "        if free_gb >= min_required:\n",
    "            return True\n",
    "        print(f\"    Waiting for RAM... ({free_gb:.1f} GB free)\")\n",
    "        gc.collect()\n",
    "        time.sleep(5)\n",
    "    return False\n",
    "\n",
    "def get_adaptive_tile_size_by_pixels(num_pixels):\n",
    "    \"\"\"Pixel-based adaptive tile sizing to prevent VS Code crashes\"\"\"\n",
    "    if num_pixels > ULTRA_PIXEL_THRESHOLD:\n",
    "        return TILE_SIZE_ULTRA, \"ULTRA\"\n",
    "    elif num_pixels > MONSTER_PIXEL_THRESHOLD:\n",
    "        return TILE_SIZE_MONSTER, \"MONSTER\"\n",
    "    else:\n",
    "        return TILE_SIZE_NORMAL, \"NORMAL\"\n",
    "\n",
    "print(\"\u00e2\u0153\u201c Checkpoint & RAM management ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SORT BY SIZE (PIXEL-BASED CATEGORIZATION) ===\n",
    "sample_sizes = []\n",
    "for sample_id in ALL_SAMPLES:\n",
    "    if sample_id in SKIP_SAMPLES:\n",
    "        continue\n",
    "    input_path = DATA_ROOT / sample_id / \"AF_removal\" / \"fused_decon_AF_cleaned.ome.tif\"\n",
    "    if input_path.exists():\n",
    "        size_mb = input_path.stat().st_size / (1024 * 1024)\n",
    "        # Get image dimensions to determine tile size\n",
    "        try:\n",
    "            with tifffile.TiffFile(input_path) as tif:\n",
    "                img_shape = tif.series[0].shape\n",
    "                if len(img_shape) == 3:  # (C, H, W)\n",
    "                    h, w = img_shape[1], img_shape[2]\n",
    "                elif len(img_shape) == 2:  # (H, W)\n",
    "                    h, w = img_shape\n",
    "                else:\n",
    "                    h, w = 0, 0\n",
    "                num_pixels = h * w\n",
    "                tile_size, category = get_adaptive_tile_size_by_pixels(num_pixels)\n",
    "                sample_sizes.append((sample_id, size_mb, tile_size, category, num_pixels, h, w))\n",
    "        except Exception as e:\n",
    "            print(f\"\u00e2\u0161\u00a0\u00ef\u00b8\u008f  {sample_id}: Could not read dimensions: {e}\")\n",
    "            sample_sizes.append((sample_id, size_mb, 1024, \"UNKNOWN\", 0, 0, 0))\n",
    "    else:\n",
    "        sample_sizes.append((sample_id, float('inf'), 0, \"MISSING\", 0, 0, 0))\n",
    "\n",
    "sample_sizes.sort(key=lambda x: x[1])\n",
    "SORTED_SAMPLES = [s[0] for s in sample_sizes]\n",
    "\n",
    "print(\"Samples (smallest first):\")\n",
    "for sid, size, tiles, cat, npx, h, w in sample_sizes:\n",
    "    if size == float('inf'):\n",
    "        print(f\"  {sid}: MISSING\")\n",
    "    elif npx > 0:\n",
    "        mpx = npx / 1_000_000\n",
    "        print(f\"  {sid}: {size:.1f} MB, {h}\u00c3\u2014{w} ({mpx:.1f}M px) \u00e2\u2020\u2019 {tiles}px tiles ({cat})\")\n",
    "    else:\n",
    "        print(f\"  {sid}: {size:.1f} MB \u00e2\u2020\u2019 {tiles}px tiles ({cat})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD MODEL ===\n",
    "check_ram(context=\"before model load\")\n",
    "print(f\"Loading {MODEL_TYPE}...\")\n",
    "predictor = get_sam_model(model_type=MODEL_TYPE, checkpoint_path=None, device='cpu')\n",
    "print(\"\u00e2\u0153\u201c Model loaded\")\n",
    "check_ram(context=\"after model load\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HELPER FUNCTIONS ===\n",
    "\n",
    "def rle_encode(mask):\n",
    "    \"\"\"RLE encoding for boolean mask.\"\"\"\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return runs\n",
    "\n",
    "def rle_decode(rle, shape):\n",
    "    \"\"\"Decode RLE to boolean mask.\"\"\"\n",
    "    starts, lengths = rle[::2], rle[1::2]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    mask = np.zeros(shape[0] * shape[1], dtype=bool)\n",
    "    for start, end in zip(starts, ends):\n",
    "        mask[start:end] = True\n",
    "    return mask.reshape(shape)\n",
    "\n",
    "\n",
    "def find_laminin_channel(sample_dir, sample_id):\n",
    "    marker_csv = sample_dir / f\"markers_{sample_id.split('_')[-1]}.csv\"\n",
    "    if not marker_csv.exists():\n",
    "        candidates = list(sample_dir.glob(\"markers_*.csv\"))\n",
    "        if not candidates:\n",
    "            return None, None, \"Marker CSV not found\"\n",
    "        marker_csv = candidates[0]\n",
    "    \n",
    "    try:\n",
    "        markers_df = pd.read_csv(marker_csv)\n",
    "    except Exception as e:\n",
    "        return None, None, f\"CSV error: {e}\"\n",
    "    \n",
    "    marker_col = None\n",
    "    for col in ['Marker-Name', 'marker_name', 'Marker', 'marker', 'name', 'Name']:\n",
    "        if col in markers_df.columns:\n",
    "            marker_col = col\n",
    "            break\n",
    "    \n",
    "    if not marker_col:\n",
    "        return None, None, \"Marker column not found\"\n",
    "    \n",
    "    laminin_rows = markers_df[markers_df[marker_col].fillna('').str.upper().str.contains('LAMININ')]\n",
    "    if laminin_rows.empty:\n",
    "        return None, None, \"Laminin not found\"\n",
    "    \n",
    "    include_col = None\n",
    "    for col in ['Include', 'include', 'included', 'Included']:\n",
    "        if col in markers_df.columns:\n",
    "            include_col = col\n",
    "            break\n",
    "    \n",
    "    if include_col:\n",
    "        included = laminin_rows[laminin_rows[include_col] == True]\n",
    "        row = included.iloc[0] if not included.empty else laminin_rows.iloc[0]\n",
    "    else:\n",
    "        row = laminin_rows.iloc[0]\n",
    "    \n",
    "    csv_pos = markers_df.index.get_loc(row.name)\n",
    "    \n",
    "    if include_col:\n",
    "        mask = markers_df[include_col] == True\n",
    "        idx = len(markers_df.iloc[:csv_pos][mask.iloc[:csv_pos]])\n",
    "    else:\n",
    "        idx = csv_pos\n",
    "    \n",
    "    return idx, row[marker_col], None\n",
    "\n",
    "\n",
    "def extract_channel_memmap(tiff_path, channel_idx):\n",
    "    try:\n",
    "        with tifffile.TiffFile(tiff_path) as tif:\n",
    "            series = tif.series[0]\n",
    "            if channel_idx >= len(series):\n",
    "                raise ValueError(f\"Channel {channel_idx} out of range\")\n",
    "            \n",
    "            page = series.pages[channel_idx]\n",
    "            img = page.asarray()\n",
    "            \n",
    "            img_float = img.astype(np.float32)\n",
    "            del img\n",
    "            gc.collect()\n",
    "            \n",
    "            vmin, vmax = np.percentile(img_float, [1, 99.5])\n",
    "            img_norm = np.clip((img_float - vmin) / (vmax - vmin), 0, 1)\n",
    "            del img_float\n",
    "            gc.collect()\n",
    "            \n",
    "            img_uint8 = (img_norm * 255).astype(np.uint8)\n",
    "            del img_norm\n",
    "            gc.collect()\n",
    "            \n",
    "            return img_uint8, None\n",
    "    except Exception as e:\n",
    "        return None, f\"Channel extraction error: {e}\"\n",
    "\n",
    "\n",
    "def segment_with_tiles_rle_compressed(image_uint8, predictor, sample_id, tile_size, overlap=256):\n",
    "    \"\"\"Segment tiles and save with RLE compression.\"\"\"\n",
    "    h, w = image_uint8.shape\n",
    "    n_y = int(np.ceil(h / (tile_size - overlap)))\n",
    "    n_x = int(np.ceil(w / (tile_size - overlap)))\n",
    "    total = n_y * n_x\n",
    "    \n",
    "    print(f\"    Tiles: {total} ({n_y}\u00c3\u2014{n_x} at {tile_size}px)\")\n",
    "    \n",
    "    temp_sample = TEMP_DIR / sample_id\n",
    "    temp_sample.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    tile_files = []\n",
    "    skipped = 0\n",
    "    \n",
    "    with tqdm(total=total, desc=\"  Segmenting tiles\", leave=False) as pbar:\n",
    "        for i in range(n_y):\n",
    "            for j in range(n_x):\n",
    "                # Check RAM every 5 tiles\n",
    "                if (i * n_x + j) % 5 == 0:\n",
    "                    free_gb = check_ram(MIN_SAFE_RAM_GB, f\"tile [{i},{j}]\")\n",
    "                    if free_gb < MIN_SAFE_RAM_GB:\n",
    "                        print(f\"\\n    Waiting for RAM...\")\n",
    "                        wait_for_ram(MIN_SAFE_RAM_GB)\n",
    "                \n",
    "                y0 = i * (tile_size - overlap)\n",
    "                x0 = j * (tile_size - overlap)\n",
    "                y1 = min(y0 + tile_size, h)\n",
    "                x1 = min(x0 + tile_size, w)\n",
    "                \n",
    "                tile = image_uint8[y0:y1, x0:x1].copy()\n",
    "                \n",
    "                try:\n",
    "                    gen = AutomaticMaskGenerator(predictor)\n",
    "                    gen.initialize(tile)\n",
    "                    masks = gen.generate()\n",
    "                    \n",
    "                    # RLE compress each mask\n",
    "                    compact_masks = []\n",
    "                    for m in masks:\n",
    "                        seg_tile = m['segmentation']  # bool array in tile coords\n",
    "                        bbox_tile = m['bbox']  # [x, y, w, h] in tile coords\n",
    "                        \n",
    "                        # Extract bbox region only\n",
    "                        x_t, y_t, w_t, h_t = bbox_tile\n",
    "                        seg_bbox = seg_tile[y_t:y_t+h_t, x_t:x_t+w_t]\n",
    "                        \n",
    "                        # RLE encode\n",
    "                        rle = rle_encode(seg_bbox)\n",
    "                        \n",
    "                        # Store compact data\n",
    "                        compact_masks.append({\n",
    "                            'rle': rle,\n",
    "                            'bbox_shape': (h_t, w_t),\n",
    "                            'bbox_global': [x_t + x0, y_t + y0, w_t, h_t],\n",
    "                            'area': m['area'],\n",
    "                            'predicted_iou': m.get('predicted_iou', 0.0)\n",
    "                        })\n",
    "                    \n",
    "                    # Save compressed tile\n",
    "                    pkl_file = temp_sample / f\"tile_{i:03d}_{j:03d}.pkl\"\n",
    "                    with open(pkl_file, 'wb') as f:\n",
    "                        pickle.dump(compact_masks, f, protocol=4)\n",
    "                    tile_files.append(pkl_file)\n",
    "                    \n",
    "                    del masks, gen, tile, compact_masks, seg_tile, seg_bbox\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    skipped += 1\n",
    "                    print(f\"\\n    \u00e2\u0161\u00a0\u00ef\u00b8\u008f  Tile [{i},{j}]: {type(e).__name__}: {str(e)[:80]}\")\n",
    "                    del tile\n",
    "                    gc.collect()\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    if skipped > 0:\n",
    "        print(f\"    \u00e2\u0161\u00a0\u00ef\u00b8\u008f  {skipped} tiles skipped\")\n",
    "    \n",
    "    return tile_files, (h, w)\n",
    "\n",
    "\n",
    "def load_tiles_chunked(tile_files, img_shape, chunk_size=None):\n",
    "    \"\"\"Load and decode RLE tiles in chunks to avoid RAM overflow.\"\"\"\n",
    "    h, w = img_shape\n",
    "    \n",
    "    # Auto-determine chunk size based on image size\n",
    "    if chunk_size is None:\n",
    "        img_pixels = h * w\n",
    "        if img_pixels > 80_000_000:  # >80M pixels (e.g., 9500\u00c3\u20149500)\n",
    "            chunk_size = TILE_LOAD_BATCH_ULTRA\n",
    "        elif img_pixels > 60_000_000:  # >60M pixels (e.g., 10000\u00c3\u20147000)\n",
    "            chunk_size = TILE_LOAD_BATCH_MONSTER\n",
    "        else:\n",
    "            chunk_size = TILE_LOAD_BATCH\n",
    "    \n",
    "    all_masks = []\n",
    "    n_chunks = int(np.ceil(len(tile_files) / chunk_size))\n",
    "    \n",
    "    print(f\"    Loading {len(tile_files)} tiles in {n_chunks} chunks of {chunk_size}...\")\n",
    "    \n",
    "    for chunk_idx in range(n_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, len(tile_files))\n",
    "        chunk_files = tile_files[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"      Chunk {chunk_idx+1}/{n_chunks}: {len(chunk_files)} tiles\")\n",
    "        \n",
    "        for pkl_file in chunk_files:\n",
    "            try:\n",
    "                with open(pkl_file, 'rb') as f:\n",
    "                    compact_masks = pickle.load(f)\n",
    "                \n",
    "                # Decode each mask\n",
    "                for cm in compact_masks:\n",
    "                    # Decode RLE to bbox-sized mask\n",
    "                    seg_bbox = rle_decode(cm['rle'], cm['bbox_shape'])\n",
    "                    \n",
    "                    # Create full-size sparse mask\n",
    "                    seg_full = np.zeros((h, w), dtype=bool)\n",
    "                    x_g, y_g, w_g, h_g = cm['bbox_global']\n",
    "                    seg_full[y_g:y_g+h_g, x_g:x_g+w_g] = seg_bbox\n",
    "                    \n",
    "                    all_masks.append({\n",
    "                        'segmentation': seg_full,\n",
    "                        'bbox': cm['bbox_global'],\n",
    "                        'area': cm['area'],\n",
    "                        'predicted_iou': cm['predicted_iou'],\n",
    "                        'stability_score': 0.0\n",
    "                    })\n",
    "                    \n",
    "                    del seg_bbox, seg_full\n",
    "                \n",
    "                del compact_masks\n",
    "                pkl_file.unlink()  # Delete as we go\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      \u00e2\u0161\u00a0\u00ef\u00b8\u008f  Load {pkl_file.name}: {e}\")\n",
    "        \n",
    "        # GC after each chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    return all_masks\n",
    "\n",
    "\n",
    "def masks_to_segmentation_chunked(masks, img_shape, batch_size=None, min_object_size=MIN_OBJECT_SIZE):\n",
    "    \"\"\"Process masks in batches to avoid RAM explosion.\"\"\"\n",
    "    h, w = img_shape\n",
    "    \n",
    "    # Auto-determine batch size based on image size\n",
    "    if batch_size is None:\n",
    "        img_pixels = h * w\n",
    "        if img_pixels > 80_000_000:  # >80M pixels\n",
    "            batch_size = MASK_BATCH_SIZE_ULTRA\n",
    "        elif img_pixels > 60_000_000:  # >60M pixels\n",
    "            batch_size = MASK_BATCH_SIZE_MONSTER\n",
    "        else:\n",
    "            batch_size = MASK_BATCH_SIZE\n",
    "    \n",
    "    print(f\"    Converting {len(masks)} masks in batches of {batch_size}...\")\n",
    "    \n",
    "    final_seg = np.zeros((h, w), dtype=np.uint32)\n",
    "    current_label = 1\n",
    "    \n",
    "    num_batches = int(np.ceil(len(masks) / batch_size))\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(masks))\n",
    "        batch_masks = masks[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"      Batch {batch_idx+1}/{num_batches}: {len(batch_masks)} masks\")\n",
    "        \n",
    "        # Check RAM\n",
    "        free_gb = check_ram(MIN_SAFE_RAM_GB, f\"batch {batch_idx+1}\")\n",
    "        if free_gb < MIN_SAFE_RAM_GB:\n",
    "            wait_for_ram(MIN_SAFE_RAM_GB)\n",
    "        \n",
    "        try:\n",
    "            # Convert batch\n",
    "            batch_seg = mask_data_to_segmentation(\n",
    "                batch_masks, \n",
    "                with_background=True, \n",
    "                min_object_size=min_object_size\n",
    "            )\n",
    "            \n",
    "            # Merge into final\n",
    "            for label_id in np.unique(batch_seg):\n",
    "                if label_id == 0:\n",
    "                    continue\n",
    "                mask = batch_seg == label_id\n",
    "                final_seg[mask & (final_seg == 0)] = current_label\n",
    "                current_label += 1\n",
    "            \n",
    "            del batch_seg, batch_masks\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      \u00e2\u0161\u00a0\u00ef\u00b8\u008f  Batch {batch_idx+1} failed: {e}\")\n",
    "            del batch_masks\n",
    "            gc.collect()\n",
    "            continue\n",
    "    \n",
    "    print(f\"    \u00e2\u2020\u2019 {current_label - 1} objects\")\n",
    "    return final_seg\n",
    "\n",
    "\n",
    "def filter_by_size_shape(seg, min_a, max_a, max_e):\n",
    "    props = regionprops(seg)\n",
    "    filt = np.zeros_like(seg)\n",
    "    for p in props:\n",
    "        if min_a <= p.area <= max_a and p.eccentricity <= max_e:\n",
    "            filt[seg == p.label] = p.label\n",
    "    return label(filt > 0)\n",
    "\n",
    "\n",
    "def apply_morphological_refinement(mask, min_a, merge_r=15, smooth_r=5):\n",
    "    binary = mask > 0\n",
    "    binary = morphology.binary_closing(binary, morphology.disk(merge_r))\n",
    "    binary = ndi.binary_fill_holes(binary)\n",
    "    binary = morphology.binary_closing(binary, morphology.disk(smooth_r))\n",
    "    binary = morphology.remove_small_objects(binary, min_size=min_a)\n",
    "    return label(binary)\n",
    "\n",
    "\n",
    "def save_outputs(final_mask, output_dir, sample_id, marker_name):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    bin_path = output_dir / \"crypt_mask_microsam.tif\"\n",
    "    lab_path = output_dir / \"crypt_mask_microsam_labeled.tif\"\n",
    "    tifffile.imwrite(bin_path, ((final_mask > 0).astype(np.uint8) * 255), compression='zlib')\n",
    "    tifffile.imwrite(lab_path, final_mask.astype(np.uint16), compression='zlib')\n",
    "    \n",
    "    reg_path = output_dir / \"segment_registry.csv\"\n",
    "    entries = []\n",
    "    for cid in np.unique(final_mask[final_mask > 0]):\n",
    "        entries.append({\n",
    "            'segment_id': int(cid),\n",
    "            'segment_type': 'crypt_microsam',\n",
    "            'mask_file': lab_path.name,\n",
    "            'global_unique_id': f\"M_{int(cid)}\",\n",
    "            'sample_id': sample_id,\n",
    "            'marker': marker_name,\n",
    "            'method': 'MicroSAM'\n",
    "        })\n",
    "    \n",
    "    reg_df = pd.DataFrame(entries)\n",
    "    if reg_path.exists():\n",
    "        exist = pd.read_csv(reg_path)\n",
    "        exist = exist[~((exist['segment_type'] == 'crypt_microsam') & (exist['sample_id'] == sample_id))]\n",
    "        reg_df = pd.concat([exist, reg_df], ignore_index=True)\n",
    "    reg_df.to_csv(reg_path, index=False)\n",
    "    \n",
    "    geo_path = output_dir / f\"{sample_id}_crypts_microsam_qupath.geojson\"\n",
    "    features = []\n",
    "    for p in regionprops(final_mask):\n",
    "        cb = (final_mask == p.label).astype(np.uint8)\n",
    "        contours = find_contours(cb, level=0.5)\n",
    "        if not contours:\n",
    "            continue\n",
    "        cnt = max(contours, key=len)\n",
    "        coords = [[float(x), float(y)] for y, x in cnt]\n",
    "        if coords[0] != coords[-1]:\n",
    "            coords.append(coords[0])\n",
    "        \n",
    "        features.append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"id\": f\"M_{p.label}\",\n",
    "            \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [coords]},\n",
    "            \"properties\": {\n",
    "                \"classification\": {\"name\": \"Crypt\", \"colorRGB\": -3140401},\n",
    "                \"object_type\": \"annotation\",\n",
    "                \"name\": f\"Crypt_{p.label}\",\n",
    "                \"isLocked\": False,\n",
    "                \"measurements\": {\n",
    "                    \"Area_um2\": float(p.area),\n",
    "                    \"Perimeter_um\": float(p.perimeter),\n",
    "                    \"Eccentricity\": float(p.eccentricity),\n",
    "                    \"Solidity\": float(p.solidity)\n",
    "                },\n",
    "                \"segment_id\": int(p.label),\n",
    "                \"global_id\": f\"M_{p.label}\",\n",
    "                \"sample_id\": sample_id,\n",
    "                \"method\": \"MicroSAM\"\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    with open(geo_path, 'w') as f:\n",
    "        json.dump({\"type\": \"FeatureCollection\", \"features\": features}, f, indent=2)\n",
    "    \n",
    "print(\"\u00e2\u0153\u201c Helper functions ready (RLE + triple-chunked: Normal/Monster/Ultra)\")\n",
    "print(\"\u00e2\u0153\u201c Helper functions ready (RLE + double-chunked processing)\")\n",
    "print(\"\u00e2\u0153\u201c Helper functions ready (RLE + double-chunked processing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BATCH PROCESSING V8 ===\n",
    "\n",
    "checkpoint = load_checkpoint()\n",
    "remaining = get_remaining_samples(checkpoint)\n",
    "remaining_sorted = [s for s in SORTED_SAMPLES if s in remaining]\n",
    "\n",
    "if not remaining_sorted:\n",
    "    print(\"\\n\u00e2\u0153\u201c ALL SAMPLES COMPLETED!\\n\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"BATCH V8 - {len(remaining_sorted)} SAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Remaining: {remaining_sorted}\\n\")\n",
    "    \n",
    "    overall_start = time.time()\n",
    "    \n",
    "    for idx, sample_id in enumerate(remaining_sorted, 1):\n",
    "        print(f\"\\n[{idx}/{len(remaining_sorted)}] {sample_id}...\")\n",
    "        \n",
    "        # RAM GATE\n",
    "        free_gb = check_ram(MIN_FREE_RAM_GB, \"start gate\")\n",
    "        if free_gb < MIN_FREE_RAM_GB:\n",
    "            print(f\"  \u00e2\u0153\u2014 SKIP: Insufficient RAM ({free_gb:.1f} GB)\")\n",
    "            print(\"    Please restart kernel\")\n",
    "            checkpoint['results'].append({\n",
    "                'sample_id': sample_id,\n",
    "                'status': 'failed',\n",
    "                'num_crypts': 0,\n",
    "                'time_seconds': 0,\n",
    "                'error': f'Insufficient RAM: {free_gb:.1f} GB'\n",
    "            })\n",
    "            save_checkpoint(checkpoint)\n",
    "            continue\n",
    "        \n",
    "        sample_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            sample_dir = DATA_ROOT / sample_id\n",
    "            input_path = sample_dir / \"AF_removal\" / \"fused_decon_AF_cleaned.ome.tif\"\n",
    "            output_dir = sample_dir / \"crypt_segmentation\"\n",
    "            \n",
    "            if not input_path.exists():\n",
    "                print(\"  \u00e2\u0161\u00a0\u00ef\u00b8\u008f  SKIP: Input missing\")\n",
    "                checkpoint['results'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'status': 'failed',\n",
    "                    'num_crypts': 0,\n",
    "                    'time_seconds': time.time() - sample_start,\n",
    "                    'error': 'Input missing'\n",
    "                })\n",
    "                save_checkpoint(checkpoint)\n",
    "                continue\n",
    "            \n",
    "            size_mb = input_path.stat().st_size / (1024 * 1024)\n",
    "            \n",
    "            # Get image dimensions for pixel-based tile sizing\n",
    "            with tifffile.TiffFile(input_path) as tif:\n",
    "                img_shape = tif.series[0].shape\n",
    "                if len(img_shape) == 3:  # (C, H, W)\n",
    "                    h, w = img_shape[1], img_shape[2]\n",
    "                else:  # (H, W)\n",
    "                    h, w = img_shape\n",
    "                num_pixels = h * w\n",
    "            \n",
    "            tile_size, category = get_adaptive_tile_size_by_pixels(num_pixels)\n",
    "            mpx = num_pixels / 1_000_000\n",
    "            print(f\"  File: {size_mb:.1f} MB, {h}\u00c3\u2014{w} ({mpx:.1f}M px) \u00e2\u2020\u2019 {tile_size}px tiles ({category})\")\n",
    "            \n",
    "            print(\"  Finding Laminin...\")\n",
    "            ch_idx, marker, error = find_laminin_channel(sample_dir, sample_id)\n",
    "            if error:\n",
    "                print(f\"  \u00e2\u0161\u00a0\u00ef\u00b8\u008f  SKIP: {error}\")\n",
    "                checkpoint['results'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'status': 'failed',\n",
    "                    'num_crypts': 0,\n",
    "                    'time_seconds': time.time() - sample_start,\n",
    "                    'error': error\n",
    "                })\n",
    "                save_checkpoint(checkpoint)\n",
    "                continue\n",
    "            print(f\"    '{marker}' at ch{ch_idx}\")\n",
    "            \n",
    "            print(\"  Extracting channel (memory-mapped)...\")\n",
    "            img, error = extract_channel_memmap(input_path, ch_idx)\n",
    "            if error:\n",
    "                print(f\"  \u00e2\u0161\u00a0\u00ef\u00b8\u008f  SKIP: {error}\")\n",
    "                checkpoint['results'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'status': 'failed',\n",
    "                    'num_crypts': 0,\n",
    "                    'time_seconds': time.time() - sample_start,\n",
    "                    'error': error\n",
    "                })\n",
    "                save_checkpoint(checkpoint)\n",
    "                continue\n",
    "            \n",
    "            print(f\"    {img.shape[0]}\u00c3\u2014{img.shape[1]} px\")\n",
    "            img_shape = img.shape\n",
    "            \n",
    "            # PHASE 1: Segment & compress tiles\n",
    "            print(f\"  Segmenting with {tile_size}px tiles (RLE compression)...\")\n",
    "            tile_files, _ = segment_with_tiles_rle_compressed(img, predictor, sample_id, tile_size, OVERLAP)\n",
    "            print(f\"    \u00e2\u2020\u2019 {len(tile_files)} tiles saved\")\n",
    "            \n",
    "            del img\n",
    "            gc.collect()\n",
    "            \n",
    "            if len(tile_files) == 0:\n",
    "                print(\"  \u00e2\u0161\u00a0\u00ef\u00b8\u008f  SKIP: No tiles\")\n",
    "                checkpoint['results'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'status': 'failed',\n",
    "                    'num_crypts': 0,\n",
    "                    'time_seconds': time.time() - sample_start,\n",
    "                    'error': 'No tiles'\n",
    "                })\n",
    "                save_checkpoint(checkpoint)\n",
    "                continue\n",
    "            \n",
    "            # PHASE 2: Load tiles in chunks (auto-adaptive chunk size)\n",
    "            print(\"  Loading tiles (chunked + RLE decode)...\")\n",
    "            masks = load_tiles_chunked(tile_files, img_shape)  # Auto-selects chunk size\n",
    "            print(f\"    \u00e2\u2020\u2019 {len(masks)} masks loaded\")\n",
    "            \n",
    "            if len(masks) == 0:\n",
    "                print(\"  \u00e2\u0161\u00a0\u00ef\u00b8\u008f  SKIP: No masks\")\n",
    "                checkpoint['results'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'status': 'failed',\n",
    "                    'num_crypts': 0,\n",
    "                    'time_seconds': time.time() - sample_start,\n",
    "                    'error': 'No masks'\n",
    "                })\n",
    "                save_checkpoint(checkpoint)\n",
    "                continue\n",
    "            # PHASE 3: Convert to segmentation in batches (auto-adaptive)\n",
    "            print(\"  Converting to segmentation (chunked batches)...\")\n",
    "            seg = masks_to_segmentation_chunked(masks, img_shape)  # Auto-selects batch size\n",
    "            \n",
    "            del masks\n",
    "            gc.collect()\n",
    "            \n",
    "            print(\"  Filtering by size & shape...\")\n",
    "            filt = filter_by_size_shape(seg, MIN_AREA, MAX_AREA, MAX_ECCENTRICITY)\n",
    "            print(f\"    \u00e2\u2020\u2019 {int(filt.max())} crypts\")\n",
    "            \n",
    "            del seg\n",
    "            gc.collect()\n",
    "            \n",
    "            if APPLY_MORPHOLOGY:\n",
    "                print(\"  Morphological refinement...\")\n",
    "                final = apply_morphological_refinement(filt, MIN_AREA, MERGE_RADIUS, SMOOTH_RADIUS)\n",
    "                n_final = int(final.max())\n",
    "                print(f\"    \u00e2\u2020\u2019 {n_final} crypts\")\n",
    "            else:\n",
    "                final = filt\n",
    "                n_final = int(final.max())\n",
    "            \n",
    "            del filt\n",
    "            gc.collect()\n",
    "            \n",
    "            print(\"  Saving outputs...\")\n",
    "            n_crypts = save_outputs(final, output_dir, sample_id, marker)\n",
    "            \n",
    "            del final\n",
    "            gc.collect()\n",
    "            \n",
    "            elapsed = time.time() - sample_start\n",
    "            print(f\"  \u00e2\u0153\u2026 {n_crypts} crypts in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "            \n",
    "            checkpoint['results'].append({\n",
    "                'sample_id': sample_id,\n",
    "                'status': 'success',\n",
    "                'num_crypts': n_crypts,\n",
    "                'time_seconds': elapsed,\n",
    "                'file_size_mb': size_mb,\n",
    "                'tile_size': tile_size,\n",
    "                'category': category,\n",
    "                'marker': marker\n",
    "            })\n",
    "            save_checkpoint(checkpoint)\n",
    "            \n",
    "            # Cleanup temp\n",
    "            try:\n",
    "                temp_sample = TEMP_DIR / sample_id\n",
    "                if temp_sample.exists():\n",
    "                    shutil.rmtree(temp_sample)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # RECOVERY PAUSE\n",
    "            if idx < len(remaining_sorted):\n",
    "                print(f\"\\n  \u00e2\u008f\u00b8  Pause {PAUSE_BETWEEN_SAMPLES}s for GC...\")\n",
    "                gc.collect()\n",
    "                time.sleep(PAUSE_BETWEEN_SAMPLES)\n",
    "            \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - sample_start\n",
    "            error_msg = f\"{type(e).__name__}: {e}\"\n",
    "            print(f\"  \u00e2\u0153\u2014 ERROR: {error_msg}\")\n",
    "            checkpoint['results'].append({\n",
    "                'sample_id': sample_id,\n",
    "                'status': 'failed',\n",
    "                'num_crypts': 0,\n",
    "                'time_seconds': elapsed,\n",
    "                'error': error_msg\n",
    "            })\n",
    "            save_checkpoint(checkpoint)\n",
    "        \n",
    "        finally:\n",
    "            gc.collect()\n",
    "    \n",
    "    total_time = time.time() - overall_start\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total: {total_time/60:.1f} min ({total_time/3600:.1f} h)\")\n",
    "    \n",
    "    success = [r for r in checkpoint['results'] if r.get('status') == 'success']\n",
    "    failed = [r for r in checkpoint['results'] if r.get('status') == 'failed']\n",
    "    \n",
    "    print(f\"Success: {len(success)}\")\n",
    "    print(f\"Failed: {len(failed)}\")\n",
    "    \n",
    "    if failed:\n",
    "        print(\"\\nFailed:\")\n",
    "        for r in failed:\n",
    "            print(f\"  - {r['sample_id']}: {r.get('error', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUMMARY ===\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"V8 BATCH SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    results = data.get('results', [])\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        print(df.to_string())\n",
    "        \n",
    "        csv_path = Path(r\"C:\\Users\\researcher\\Downloads\\Cycif_pipeline_V3\\microsam_batch_summary_v8.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n\u00e2\u0153\u201c Saved: {csv_path}\")\n",
    "    else:\n",
    "        print(\"No results.\")\n",
    "else:\n",
    "    print(\"No checkpoint.\")\n",
    "\n",
    "# Cleanup temp\n",
    "if TEMP_DIR.exists():\n",
    "    try:\n",
    "        shutil.rmtree(TEMP_DIR)\n",
    "        print(f\"\\n\u00e2\u0153\u201c Cleaned: {TEMP_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u00e2\u0161\u00a0\u00ef\u00b8\u008f  Cleanup: {e}\")\n",
    "\n",
    "print(f\"\\nFinal RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB free\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microsam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}