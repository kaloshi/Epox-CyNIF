{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da41b5f3",
   "metadata": {},
   "source": [
    "# CyLinter Pipeline - GUI-basierter Workflow\n",
    "\n",
    "**Dieses Notebook steuert CyLinter QC ausschlie?lich ?ber GUIs. Konfiguration (Zelle 8) ? Ausf?hrung (Zelle 9); Codezellen bleiben unver?ndert.**\n",
    "\n",
    "---\n",
    "\n",
    "## ?? Notebook-?berblick\n",
    "| # | Typ | Zweck | Pflicht? |\n",
    "|---|-----|-------|----------|\n",
    "|1|MD|?bersicht & Workflow-Hinweise|??|\n",
    "|2|Code|Setup & Pfade (BASE_DIR, CONFIG_PATH, markers.csv)|? Run All|\n",
    "|3|Code|`cylinter_config.yml` + `markers.csv` laden/validieren|? Run All|\n",
    "|4|MD|Kurze Erkl?rung zur Config|??|\n",
    "|5|Code|Manuelle Quantifizierung & CSV-Erstellung|?? Optional|\n",
    "|6|Code|Pre-Flight Check (Pfad- & Schema-Validierung)|? Run All|\n",
    "|7|MD|Intro zur Konfigurations-GUI|??|\n",
    "|8|Code|**?? Konfig-GUI** (Marker-Subset w?hlen & speichern)|?? Tag 1|\n",
    "|9|Code|**?? PIPELINE-START** (Checkpoint-aware)|?? Tag 1|\n",
    "|10|Code|**STOP** ? Tag?1 endet hier|??|\n",
    "|11|MD|Intro zur erweiterten Steuerung|??|\n",
    "|12|Code|**?? Erweiterte GUI** (Tag?2+ / Fehlerkorrektur)|?? Tag 2+|\n",
    "|13|MD|Hinweis zum Parquet-Viewer|?? Optional|\n",
    "|14|Code|Parquet Viewer (Checkpoint-Dateien inspizieren)|?? Optional|\n",
    "\n",
    "---\n",
    "\n",
    "## ?? WORKFLOW TAG 1 (Erste Analyse)\n",
    "\n",
    "### 1?? Notebook starten\n",
    "```\n",
    "Kernel ? Restart & Run All\n",
    "```\n",
    "- ? F?hrt Zellen 2?6 aus (Setup, Config-Load, optionale manuelle Quantifizierung, Pre-Flight-Check).\n",
    "- ? L?dt Konfigurations-GUI (Zelle 8) und Pipeline-Start (Zelle 9).\n",
    "- ? **Stoppt an der STOP-Zelle** (verhindert versehentlichen Tag-2-Start).\n",
    "\n",
    "### 2?? Marker ausw?hlen (Zelle 8)\n",
    "**Quick-Optionen:** Alle Marker, Alle au?er AF, Nur Bio (Counts basieren auf `markers.csv`).  \n",
    "**Custom-Option:** Bio-Marker per Checkbox (AF wird automatisch ausgeschlossen).  \n",
    "**Button ??? Speichern & Bereit?:** Schreibt `markersToExclude` in `cylinter_config.yml`, startet keine Pipeline.\n",
    "\n",
    "### 3?? Pipeline starten (Zelle 9)\n",
    "- Zeigt Marker-Zusammenfassung und 15 Module.\n",
    "- **Button ??? Pipeline starten?** startet beim ersten fehlenden Checkpoint und l?uft bis `curateThumbnails`.\n",
    "- Interactive Module (`selectROIs`, `setContrast`, `gating`) ?ffnen GUIs.\n",
    "\n",
    "**?? Tag?1 Tipp:** Pipeline nicht abbrechen; Checkpoints werden nur beim vollst?ndigen Lauf geschrieben.\n",
    "\n",
    "---\n",
    "\n",
    "## ?? WORKFLOW TAG 2+ (Neue Marker oder Fehlerkorrektur)\n",
    "\n",
    "1?? Notebook neu starten, nur Zellen 2?3 ausf?hren (Setup + Config).  \n",
    "2?? Zelle 12 ?ffnen ? Erweiterte GUI.  \n",
    "3?? Tab 1: Marker erg?nzen/mergen (Backup wird erstellt).  \n",
    "4?? Tab 2: Checkpoints gezielt l?schen, Startmodul w?hlen, ?Run Selected?.  \n",
    "\n",
    "Gating nutzt vorhandene Thresholds aus `cylinter_report.yml`; nur Marker ohne Threshold werden neu gegated.\n",
    "\n",
    "---\n",
    "\n",
    "## ?? WORKFLOW FEHLERKORREKTUR (einzelnes Modul neu starten)\n",
    "\n",
    "- Notebook neu starten ? Zellen 2?3 ausf?hren.  \n",
    "- Zelle 12 (Tab 2): fehlerhaftes Modul resetten, Startmodul w?hlen ? ?Run Selected?.  \n",
    "- Pipeline l?uft bis Ende, ?berspringt bestehende Checkpoints.\n",
    "\n",
    "---\n",
    "\n",
    "## ?? Eingaben & Modul-Liste\n",
    "- Erwartet `cylinter_config.yml` und `markers.csv` im Notebook-Ordner; Checkpoints/Reports: `cylinter_output_prune_test/`.\n",
    "- Pipeline-Module (15):\n",
    "```\n",
    "1. aggregateData\n",
    "2. selectROIs (GUI)\n",
    "3. intensityFilter\n",
    "4. areaFilter\n",
    "5. cycleCorrelation\n",
    "6. logTransform\n",
    "7. pruneOutliers\n",
    "8. metaQC\n",
    "9. PCA\n",
    "10. setContrast (GUI)\n",
    "11. gating (GUI)\n",
    "12. clustering\n",
    "13. clustermap\n",
    "14. frequencyStats\n",
    "15. curateThumbnails\n",
    "```\n",
    "\n",
    "**Interactive Module (?ffnen GUIs):**\n",
    "- `selectROIs`, `setContrast`, `gating`\n",
    "\n",
    "---\n",
    "\n",
    "##  CHECKPOINT-SYSTEM VERSTEHEN\n",
    "\n",
    "**Was sind Checkpoints?**\n",
    "- Nach jedem erfolgreichen Modul speichert CyLinter eine `.parquet` Datei\n",
    "- Speicherort: `cylinter_output_prune_test/checkpoints/`\n",
    "- Format: `module_name.parquet` (z.B. `aggregateData.parquet`, `gating.parquet`)\n",
    "\n",
    "**Wie funktioniert es?**\n",
    "- Beim Pipeline-Start pr?ft CyLinter, welche Checkpoints existieren\n",
    "- Module mit Checkpoints werden **automatisch ?bersprungen**\n",
    "- Nur Module ohne Checkpoint oder mit gel?schtem Checkpoint laufen neu\n",
    "\n",
    "**Warum ist das n?tzlich?**\n",
    "- ? **Zeit sparen:** Keine unn?tigen Neuberechnungen\n",
    "- ? **Incremental Work:** Neue Marker hinzuf?gen ohne alles neu zu machen\n",
    "- ? **Fehlerkorrektur:** Einzelne Module isoliert neu starten\n",
    "- ? **Experimente:** Verschiedene Parameter testen (z.B. Clustering-Params)\n",
    "\n",
    "**Gating-Thresholds:**\n",
    "- Werden in `cylinter_report.yml` gespeichert (nicht in Checkpoint!)\n",
    "- Persistieren ?ber Pipeline-L?ufe hinweg\n",
    "- Erm?glichen TAG 2 Workflow: Neue Marker ohne alte Marker neu zu gaten\n",
    "\n",
    "---\n",
    "\n",
    "## ?? WICHTIGE HINWEISE\n",
    "\n",
    "### CyLinter's Pipeline-Verhalten:\n",
    "- **`--module X` setzt nur STARTPUNKT, nicht Endpunkt!**\n",
    "- Pipeline l?uft IMMER vom Startmodul bis zum Ende (15. Modul)\n",
    "- Sie k?nnen KEINE Modul-Teilmenge ausw?hlen (z.B. nur Module 3-5)\n",
    "- Checkpoints erm?glichen das ?berspringen: Pipeline l?uft alle 15 durch, aber existierende Checkpoints werden ?bersprungen\n",
    "\n",
    "### TAG 1 vs TAG 2:\n",
    "- **TAG 1:** Vollst?ndige Erstanalyse\n",
    "  - Zelle 8: Marker w?hlen\n",
    "  - Zelle 9: Pipeline starten (aggregateData ? Ende)\n",
    "  - Alle interactive Module durchlaufen\n",
    "  \n",
    "- **TAG 2+:** Inkrementelle Updates\n",
    "  - Zelle 12 Tab 1: Neue Marker mergen\n",
    "  - Zelle 12 Tab 2: Checkpoint l?schen + Pipeline starten\n",
    "  - Checkpoints erm?glichen ?berspringen bereits berechneter Schritte\n",
    "\n",
    "### Laufzeiten (ca.):\n",
    "- **TAG 1 (AF ausgeschlossen):** ~40-50 Min\n",
    "- **TAG 1 (alle Marker):** ~90-120 Min\n",
    "- **TAG 2 (neue Marker):** ~10-15 Min\n",
    "- **Fehlerkorrektur (1 Modul):** ~2-10 Min (je nach Modul)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970dba5b-94f0-4cc2-830a-c9200afafec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 1: Imports und Basiskonfiguration (DYNAMISCH f\u00c3\u00bcr beliebige Samples)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import time\n",
    "import os\n",
    "import subprocess # F\u00c3\u00bcr CLI-Aufrufe\n",
    "import shutil\n",
    "\n",
    "print(\"--- Zelle 1: Modulimporte, Pfaddefinitionen und automatische Dateierkennung ---\")\n",
    "BASE_DIR = Path.cwd()\n",
    "CONFIG_PATH = BASE_DIR / \"cylinter_config.yml\"\n",
    "MARKERS_CSV_PATH = BASE_DIR / \"markers.csv\"\n",
    "\n",
    "print(f\"Projekt-Basisverzeichnis (BASE_DIR): {BASE_DIR}\")\n",
    "print(f\"Konfigurationsdatei (CONFIG_PATH): {CONFIG_PATH}\")\n",
    "print(f\"Marker CSV (MARKERS_CSV_PATH): {MARKERS_CSV_PATH}\")\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"cylinter_config.yml nicht gefunden: {CONFIG_PATH}\")\n",
    "if not MARKERS_CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"markers.csv nicht gefunden: {MARKERS_CSV_PATH}\")\n",
    "\n",
    "# Sicherstellen, dass die Eingabeordner existieren\n",
    "(BASE_DIR / 'csv').mkdir(exist_ok=True)\n",
    "(BASE_DIR / 'tif').mkdir(exist_ok=True)\n",
    "(BASE_DIR / 'seg').mkdir(exist_ok=True)\n",
    "(BASE_DIR / 'mask').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n--- \u00c3\u0153berpr\u00c3\u00bcfe vorhandene Dateien ---\")\n",
    "# Z\u00c3\u00a4hle Dateien in jedem Ordner\n",
    "for folder in ['csv', 'tif', 'seg', 'mask']:\n",
    "    folder_path = BASE_DIR / folder\n",
    "    if folder_path.exists():\n",
    "        files = list(folder_path.glob('*'))\n",
    "        files = [f for f in files if f.is_file()]  # Nur Dateien, keine Ordner\n",
    "        print(f\"{folder.upper()}: {len(files)} Datei(en) gefunden\")\n",
    "        if files:\n",
    "            for f in files[:3]:  # Zeige max. 3 Dateien\n",
    "                print(f\"  - {f.name}\")\n",
    "            if len(files) > 3:\n",
    "                print(f\"  ... und {len(files)-3} weitere\")\n",
    "\n",
    "# --- OPTIONALE DATEIVORBEREITUNG ---\n",
    "# Falls .tiff Dateien vorhanden sind, diese zu .tif konvertieren\n",
    "print(\"\\n--- Pr\u00c3\u00bcfe auf .tiff Dateien (falls vorhanden) ---\")\n",
    "tiff_converted = False\n",
    "for folder in ['tif', 'seg', 'mask']:\n",
    "    folder_path = BASE_DIR / folder\n",
    "    if folder_path.exists():\n",
    "        tiff_files = list(folder_path.glob(\"*.tiff\"))\n",
    "        if tiff_files:\n",
    "            print(f\"{folder.upper()}: Konvertiere {len(tiff_files)} .tiff Datei(en)...\")\n",
    "            for tiff_file in tiff_files:\n",
    "                tif_file = tiff_file.with_suffix('.tif')\n",
    "                shutil.move(tiff_file, tif_file)\n",
    "                print(f\"   \u00e2\u2020\u2019 Konvertiert: {tiff_file.name} \u00e2\u2020\u2019 {tif_file.name}\")\n",
    "                tiff_converted = True\n",
    "        else:\n",
    "            print(f\"{folder.upper()}: Keine .tiff Dateien gefunden (OK)\")\n",
    "\n",
    "if not tiff_converted:\n",
    "    print(\"\\nKeine .tiff Konvertierung n\u00c3\u00b6tig - alle Dateien bereits im .tif Format.\")\n",
    "\n",
    "print(\"\\n--- Dateivorbereitung abgeschlossen ---\")\n",
    "\n",
    "# --- WICHTIG: markers.csv NICHT modifizieren (muss OME-TIFF Namen entsprechen) ---\n",
    "print(\"\\n--- Pr\u00c3\u00bcfe markers.csv Format ---\")\n",
    "# KRITISCH: markers.csv muss die ORIGINALEN Kanalnamen aus OME-TIFF enthalten (OHNE _cX)!\n",
    "# Die _cX Suffixe geh\u00c3\u00b6ren NUR in die CSV-Datei (Quantifizierung), NICHT in markers.csv\n",
    "\n",
    "import pandas as pd\n",
    "markers_original_path = BASE_DIR / \"markers.csv\"\n",
    "markers_backup_path = BASE_DIR / \"markers_original_backup.csv\"\n",
    "\n",
    "# Backup erstellen (nur einmal)\n",
    "if not markers_backup_path.exists():\n",
    "    import shutil\n",
    "    shutil.copy(markers_original_path, markers_backup_path)\n",
    "    print(f\"  Original markers.csv gesichert als: {markers_backup_path.name}\")\n",
    "\n",
    "# Markers.csv laden und pr\u00c3\u00bcfen\n",
    "df_markers = pd.read_csv(markers_original_path)\n",
    "print(f\"  Geladene markers.csv: {len(df_markers)} Eintr\u00c3\u00a4ge\")\n",
    "\n",
    "# CHECK & AUTO-FIX: Pr\u00c3\u00bcfe ob _cX vorhanden ist und f\u00c3\u00bcge hinzu falls n\u00c3\u00b6tig\n",
    "sample_marker_names = df_markers['marker_name'].head(3).tolist()\n",
    "has_cycle_suffix = any('_c' in str(m) for m in sample_marker_names)\n",
    "\n",
    "if not has_cycle_suffix:\n",
    "    print(f\"  \u00e2\u0161\u00a0\u00ef\u00b8\u008f  markers.csv hat KEINE _cX Suffixe: {sample_marker_names}\")\n",
    "    print(f\"  \u00e2\u2020\u2019 F\u00c3\u00bcge _cX automatisch hinzu (f\u00c3\u00bcr aggregateData)...\")\n",
    "    \n",
    "    # F\u00c3\u00bcge _cX Suffixe hinzu (z.B. \"DAPI\" \u00e2\u2020\u2019 \"DAPI_c1\")\n",
    "    df_markers['marker_name'] = (\n",
    "        df_markers['marker_name'].str.strip().str.replace(' ', '_', regex=False) + \n",
    "        '_c' + df_markers['cycle_number'].astype(str)\n",
    "    )\n",
    "    \n",
    "    # Speichern\n",
    "    df_markers.to_csv(markers_original_path, index=False)\n",
    "    print(f\"  \u00e2\u0153\u2026 markers.csv korrigiert: _cX Suffixe hinzugef\u00c3\u00bcgt\")\n",
    "    print(f\"     Beispiele: {df_markers['marker_name'].head(3).tolist()}\")\n",
    "else:\n",
    "    print(f\"  \u00e2\u0153\u2026 markers.csv hat _cX Suffixe (korrekt f\u00c3\u00bcr aggregateData): {sample_marker_names}\")\n",
    "    print(f\"  \u00e2\u2020\u2019 CyLinter aggregateData kann CSV-Spalten finden!\")\n",
    "\n",
    "# Leerzeichen entfernen (falls vorhanden)\n",
    "if df_markers['marker_name'].str.contains(' ', regex=False).any():\n",
    "    print(f\"  \u00e2\u0161\u2122\u00ef\u00b8\u008f  Entferne Leerzeichen aus Marker-Namen...\")\n",
    "    df_markers['marker_name'] = df_markers['marker_name'].str.strip().str.replace(' ', '_', regex=False)\n",
    "    df_markers.to_csv(markers_original_path, index=False)\n",
    "    print(f\"  \u00e2\u0153\u2026 Leerzeichen entfernt\")\n",
    "\n",
    "print(\"\\n--- Pr\u00c3\u00bcfung abgeschlossen ---\")\n",
    "print(\"Zelle 1 erfolgreich ausgef\u00c3\u00bchrt.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f79ff-b7b2-4ef5-b754-7fedb18c3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 2: Konfigurationsobjekt erstellen (vereinfacht f\u00c3\u00bcr Klarheit, da CLI die Config liest)\n",
    "print(\"--- Zelle 2: Lade und validiere Konfigurationsdaten aus YAML ---\")\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"FEHLER: {CONFIG_PATH} nicht gefunden. Zelle 1 erneut ausf\u00c3\u00bchren oder Pfad pr\u00c3\u00bcfen.\")\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f_yaml:\n",
    "        config_dict_from_yaml = yaml.safe_load(f_yaml) # Diese Variable wird in Zelle 2.5 verwendet\n",
    "    if config_dict_from_yaml is None:\n",
    "        raise ValueError(\"cylinter_config.yml konnte nicht geladen werden oder ist leer.\")\n",
    "    print(f\"cylinter_config.yml erfolgreich als Dictionary geladen.\")\n",
    "except Exception as e:\n",
    "    print(f\"FEHLER beim Laden der cylinter_config.yml: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    markers_df_for_quant = pd.read_csv(MARKERS_CSV_PATH)\n",
    "    if 'marker_name' not in markers_df_for_quant.columns:\n",
    "        raise ValueError(\"Spalte 'marker_name' nicht in markers.csv gefunden.\")\n",
    "    if 'cycle_number' not in markers_df_for_quant.columns:\n",
    "        raise ValueError(\"Spalte 'cycle_number' nicht in markers.csv gefunden.\")\n",
    "    \n",
    "    # WICHTIG: markers.csv enth\u00c3\u00a4lt bereits _cX Suffixe (nach Auto-Fix in Zelle 1)\n",
    "    # Verwende diese DIREKT ohne weitere Modifikation\n",
    "    all_channel_names_from_markers_csv = (\n",
    "        markers_df_for_quant['marker_name'].str.strip().str.replace(' ', '_', regex=False)\n",
    "    ).tolist()\n",
    "    \n",
    "    print(f\"Marker-Informationen aus {MARKERS_CSV_PATH} geladen. {len(all_channel_names_from_markers_csv)} Kan\u00c3\u00a4le gefunden.\")\n",
    "    print(f\"  Beispiele (bereits mit _cX aus markers.csv): {all_channel_names_from_markers_csv[:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"FEHLER beim Laden der markers.csv: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    inDir_name_from_yaml = config_dict_from_yaml.get('inDir', '.')\n",
    "    inDir_for_quant = (BASE_DIR / Path(inDir_name_from_yaml)).resolve()\n",
    "    image_dir_name_from_yaml = config_dict_from_yaml.get('image_dir', 'tif')\n",
    "    mask_dir_name_from_yaml = config_dict_from_yaml.get('mask_dir', 'mask')\n",
    "    csv_dir_name_from_yaml = config_dict_from_yaml.get('csv_dir', 'csv')\n",
    "    smd = config_dict_from_yaml.get('sampleMetadata', {})\n",
    "    if not smd: raise ValueError(\"'sampleMetadata' fehlt in Config.\")\n",
    "    sample_key_from_yaml = list(smd.keys())[0]\n",
    "    sample_values_from_yaml = smd[sample_key_from_yaml]\n",
    "    SAMPLE_NAME_FOR_FILES_QUANT = sample_values_from_yaml[0]\n",
    "    print(f\"F\u00c3\u00bcr Quantifizierung wird Sample-Dateiname verwendet: {SAMPLE_NAME_FOR_FILES_QUANT}\")\n",
    "    path_image_for_quant = inDir_for_quant / image_dir_name_from_yaml / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.ome.tif\"\n",
    "    path_cell_mask_for_quant = inDir_for_quant / mask_dir_name_from_yaml / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.tif\"\n",
    "    path_csv_output_for_quant = inDir_for_quant / csv_dir_name_from_yaml / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.csv\"\n",
    "    print(f\"  Erwarteter Pfad f\u00c3\u00bcr Bild: {path_image_for_quant}\")\n",
    "    print(f\"  Erwarteter Pfad f\u00c3\u00bcr Zellmaske: {path_cell_mask_for_quant}\")\n",
    "    print(f\"  Erwarteter Pfad f\u00c3\u00bcr Ausgabe-CSV: {path_csv_output_for_quant}\")\n",
    "except KeyError as ke: print(f\"FEHLER: Schl\u00c3\u00bcssel in config nicht gefunden: {ke}\"); raise\n",
    "except Exception as e: print(f\"FEHLER beim Extrahieren von Werten aus Config: {e}\"); raise\n",
    "print(\"Zelle 2 erfolgreich ausgef\u00c3\u00bchrt.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491730aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \u00f0\u0178\u201c\u2039 Zelle 2: Konfiguration laden\n",
    "\n",
    "L\u00c3\u00a4dt `cylinter_config.yml` und `markers.csv`, validiert Struktur und bereitet Pfade vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659056c-3922-4b15-a93d-b86f6a2de2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 2.5: Manuelle Quantifizierung und Erstellung der CSV-Datei (DYNAMISCH)\n",
    "print(\"--- Zelle 2.5: Manuelle Quantifizierung und Erstellung der CSV-Datei ---\")\n",
    "\n",
    "from skimage.measure import regionprops_table, regionprops\n",
    "import tifffile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# \u00c3\u0153berpr\u00c3\u00bcfe, ob die notwendigen Variablen aus Zelle 2 existieren\n",
    "if 'config_dict_from_yaml' not in locals():\n",
    "    raise NameError(\"config_dict_from_yaml ist nicht definiert. F\u00c3\u00bchren Sie Zelle 2 erneut aus.\")\n",
    "if 'all_channel_names_from_markers_csv' not in locals():\n",
    "    raise NameError(\"all_channel_names_from_markers_csv ist nicht definiert. F\u00c3\u00bchren Sie Zelle 2 erneut aus.\")\n",
    "if 'SAMPLE_NAME_FOR_FILES_QUANT' not in locals():\n",
    "    raise NameError(\"SAMPLE_NAME_FOR_FILES_QUANT ist nicht definiert. F\u00c3\u00bchren Sie Zelle 2 erneut aus.\")\n",
    "\n",
    "# --- DYNAMISCHE DATEIERKENNUNG (wie in Zelle 1) ---\n",
    "def find_first_file(directory, extensions, exclude_patterns=None):\n",
    "    \"\"\"Findet die erste Datei mit passender Endung\"\"\"\n",
    "    if exclude_patterns is None:\n",
    "        exclude_patterns = []\n",
    "    dir_path = BASE_DIR / directory\n",
    "    if not dir_path.exists():\n",
    "        return None\n",
    "    for ext in extensions:\n",
    "        for file in sorted(dir_path.glob(f\"*{ext}\")):\n",
    "            if any(pattern in file.name for pattern in exclude_patterns):\n",
    "                continue\n",
    "            return file\n",
    "    return None\n",
    "\n",
    "print(\"\\n--- Suche vorhandene Dateien f\u00c3\u00bcr Quantifizierung ---\")\n",
    "\n",
    "# TIF: Suche tats\u00c3\u00a4chlich vorhandene Datei\n",
    "tif_file = find_first_file('tif', ['.ome.tif', '.tif'], exclude_patterns=['.tiff'])\n",
    "if not tif_file:\n",
    "    raise FileNotFoundError(f\"FEHLER: Keine TIF-Datei in {BASE_DIR / 'tif'} gefunden!\")\n",
    "path_image_for_quant = tif_file\n",
    "print(f\"Verwende TIF: {path_image_for_quant.name}\")\n",
    "\n",
    "# MASK: Suche tats\u00c3\u00a4chlich vorhandene Datei\n",
    "mask_file = find_first_file('mask', ['.tif', '.tiff'], exclude_patterns=[])\n",
    "if not mask_file:\n",
    "    raise FileNotFoundError(f\"FEHLER: Keine Mask-Datei in {BASE_DIR / 'mask'} gefunden!\")\n",
    "path_cell_mask_for_quant = mask_file\n",
    "print(f\"Verwende MASK: {path_cell_mask_for_quant.name}\")\n",
    "\n",
    "# CSV: Ziel f\u00c3\u00bcr Ausgabe\n",
    "path_csv_output_for_quant = BASE_DIR / 'csv' / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.csv\"\n",
    "print(f\"Ausgabe CSV: {path_csv_output_for_quant.name}\")\n",
    "print(\"--- Dateien gefunden ---\\n\")\n",
    "\n",
    "markers_to_exclude_for_quant = config_dict_from_yaml.get('markersToExclude', [])\n",
    "quantification_channel_names = [\n",
    "    name for name in all_channel_names_from_markers_csv\n",
    "    if name not in markers_to_exclude_for_quant\n",
    "]\n",
    "print(f\"F\u00c3\u00bcr die Quantifizierung werden {len(quantification_channel_names)} Kan\u00c3\u00a4le verwendet (nach Ausschluss).\")\n",
    "print(f\"  Ausgeschlossene Marker f\u00c3\u00bcr Quant (aus config): {markers_to_exclude_for_quant[:5]}... (bis zu 5 gezeigt)\")\n",
    "print(f\"  Zu quantifizierende Marker: {quantification_channel_names[:5]}... (bis zu 5 gezeigt)\")\n",
    "\n",
    "# --- Bild und Maske laden ---\n",
    "print(f\"Lade Bild f\u00c3\u00bcr Quantifizierung: {path_image_for_quant}\")\n",
    "if not path_image_for_quant.exists():\n",
    "    raise FileNotFoundError(f\"Bilddatei nicht gefunden: {path_image_for_quant}\")\n",
    "full_image_data = tifffile.imread(path_image_for_quant)\n",
    "print(f\"  Bildform roh: {full_image_data.shape}\")\n",
    "\n",
    "# Bilddaten in (H, W, C) bringen, falls n\u00c3\u00b6tig (Logik von Ihnen \u00c3\u00bcbernommen)\n",
    "if full_image_data.ndim == 3 and full_image_data.shape[0] == len(all_channel_names_from_markers_csv):\n",
    "    full_image_data_hwc = np.transpose(full_image_data, (1, 2, 0))\n",
    "elif full_image_data.ndim == 3 and full_image_data.shape[2] == len(all_channel_names_from_markers_csv):\n",
    "    full_image_data_hwc = full_image_data\n",
    "elif full_image_data.ndim == 2 and len(all_channel_names_from_markers_csv) == 1:\n",
    "    full_image_data_hwc = np.expand_dims(full_image_data, axis=-1)\n",
    "else:\n",
    "    raise ValueError(f\"Unerwartete Bilddimensionen {full_image_data.shape} f\u00c3\u00bcr {len(all_channel_names_from_markers_csv)} Kan\u00c3\u00a4le.\")\n",
    "print(f\"  Bildform f\u00c3\u00bcr Quantifizierung (H,W,C): {full_image_data_hwc.shape}\")\n",
    "\n",
    "print(f\"Lade Zellmaske f\u00c3\u00bcr Quantifizierung: {path_cell_mask_for_quant}\")\n",
    "if not path_cell_mask_for_quant.exists():\n",
    "    raise FileNotFoundError(f\"Zellmaskendatei nicht gefunden: {path_cell_mask_for_quant}\")\n",
    "cell_mask = tifffile.imread(path_cell_mask_for_quant) # WICHTIG: cell_mask wird hier definiert\n",
    "print(f\"  Zellmaskenform: {cell_mask.shape}, Max Label: {np.max(cell_mask)}\")\n",
    "\n",
    "# WICHTIG: Validiere dass Image und Maske kompatible Shapes haben\n",
    "if cell_mask.shape[:2] != full_image_data_hwc.shape[:2]:\n",
    "    print(f\"\\n\u00e2\u0161\u00a0\u00ef\u00b8\u008f  WARNUNG: Shape-Mismatch!\")\n",
    "    print(f\"   Image (H,W,C): {full_image_data_hwc.shape}\")\n",
    "    print(f\"   Mask  (H,W):   {cell_mask.shape}\")\n",
    "    print(f\"\\n   \u00e2\u2020\u2019 Passe Maske an Image-Dimensionen an...\")\n",
    "    \n",
    "    from skimage.transform import resize\n",
    "    # Resize Maske auf Image-Dimensionen (nearest neighbor f\u00c3\u00bcr Labels!)\n",
    "    cell_mask_resized = resize(\n",
    "        cell_mask.astype(float), \n",
    "        full_image_data_hwc.shape[:2], \n",
    "        order=0,  # Nearest neighbor (wichtig f\u00c3\u00bcr Labels!)\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=False\n",
    "    ).astype(cell_mask.dtype)\n",
    "    cell_mask = cell_mask_resized\n",
    "    print(f\"   \u00e2\u0153\u201c Maske angepasst auf: {cell_mask.shape}\")\n",
    "\n",
    "# --- DataFrame Erstellung und Quantifizierung ---\n",
    "if np.max(cell_mask) == 0:\n",
    "    print(\"WARNUNG: Zellmaske enth\u00c3\u00a4lt keine Labels (max Label = 0). Erstelle leere CSV.\")\n",
    "    df_features = pd.DataFrame() # df_features wird hier f\u00c3\u00bcr den leeren Fall definiert\n",
    "else:\n",
    "    print(\"Berechne morphologische Eigenschaften...\")\n",
    "    features_table = regionprops_table(\n",
    "        cell_mask,\n",
    "        properties=('label', 'area', 'centroid', 'major_axis_length',\n",
    "                    'minor_axis_length', 'eccentricity', 'solidity', 'orientation')\n",
    "    )\n",
    "    df_features = pd.DataFrame(features_table) # df_features wird hier f\u00c3\u00bcr den nicht-leeren Fall definiert\n",
    "    print(f\"  Spalten direkt nach regionprops_table: {df_features.columns.tolist()}\")\n",
    "\n",
    "    df_features.rename(columns={\n",
    "        'label': 'CellID', 'area': 'Area', 'centroid-0': 'Y_centroid', 'centroid-1': 'X_centroid',\n",
    "        'major_axis_length': 'MajorAxisLength', 'minor_axis_length': 'MinorAxisLength',\n",
    "        'eccentricity': 'Eccentricity', 'solidity': 'Solidity', 'orientation': 'Orientation'\n",
    "    }, inplace=True)\n",
    "    print(f\"  Spalten nach Umbenennung: {df_features.columns.tolist()}\")\n",
    "\n",
    "    if 'Area' in df_features.columns:\n",
    "        df_features['Area'] = df_features['Area'].astype(float)\n",
    "    else:\n",
    "        print(\"WARNUNG: Spalte 'Area' nach Umbenennung immer noch nicht gefunden!\")\n",
    "\n",
    "    print(f\"\\nExtrahiere Intensit\u00c3\u00a4ten f\u00c3\u00bcr {len(quantification_channel_names)} ausgew\u00c3\u00a4hlte Marker pro Zelle...\")\n",
    "    print(\"  Strategie: Mean (Standard) + 95th Percentile (f\u00c3\u00bcr Membranmarker)\")\n",
    "    original_channel_name_to_index = {name: i for i, name in enumerate(all_channel_names_from_markers_csv)}\n",
    "    \n",
    "    # Lade markers.csv f\u00c3\u00bcr localization Info\n",
    "    markers_df_loc = pd.read_csv(MARKERS_CSV_PATH)\n",
    "    marker_localization = dict(zip(markers_df_loc['marker_name'], markers_df_loc.get('localization', ['unknown']*len(markers_df_loc))))\n",
    "    \n",
    "    # Pr\u00c3\u00bcfe ob quantificationMetrics in config vorhanden\n",
    "    compute_p95 = config_dict_from_yaml.get('quantificationMetrics', {}).get('compute_percentile_95', False)\n",
    "    \n",
    "    # Progress tracking\n",
    "    from tqdm.auto import tqdm\n",
    "    membrane_count = 0\n",
    "    progress_bar = tqdm(quantification_channel_names, desc=\"Quantifiziere Marker\", unit=\"marker\")\n",
    "    \n",
    "    for marker_name_to_quantify in progress_bar:\n",
    "        if marker_name_to_quantify not in original_channel_name_to_index:\n",
    "            print(f\"    WARNUNG: Zu quantifizierender Marker '{marker_name_to_quantify}' nicht in urspr\u00c3\u00bcnglichen Kanalnamen gefunden. \u00c3\u0153berspringe.\")\n",
    "            if not df_features.empty: df_features[marker_name_to_quantify] = np.nan # Nur hinzuf\u00c3\u00bcgen, wenn df nicht leer\n",
    "            continue\n",
    "\n",
    "        channel_index = original_channel_name_to_index[marker_name_to_quantify]\n",
    "        is_membrane = marker_localization.get(marker_name_to_quantify, 'unknown') == 'membrane'\n",
    "\n",
    "        if channel_index < full_image_data_hwc.shape[2]:\n",
    "            current_channel_image = full_image_data_hwc[:, :, channel_index]\n",
    "            \n",
    "            # Berechne MEAN (Standard f\u00c3\u00bcr alle Marker)\n",
    "            intensity_props = regionprops_table(label_image=cell_mask, intensity_image=current_channel_image,\n",
    "                                                properties=['label', 'intensity_mean'])\n",
    "            df_intensity_marker = pd.DataFrame(intensity_props)\n",
    "            df_intensity_marker.rename(columns={'label': 'CellID', 'intensity_mean': marker_name_to_quantify}, inplace=True)\n",
    "            \n",
    "            # Berechne 95th PERCENTILE f\u00c3\u00bcr Membranmarker (wenn aktiviert)\n",
    "            if compute_p95 and is_membrane:\n",
    "                membrane_count += 1\n",
    "                progress_bar.set_postfix({\"Marker\": marker_name_to_quantify[:20], \"Type\": \"membrane+p95\"})\n",
    "                \n",
    "                # OPTIMIERTE METHODE: ~5-10x schneller als Loop\n",
    "                from scipy.ndimage import labeled_comprehension\n",
    "                cell_ids = df_intensity_marker['CellID'].values\n",
    "                p95_values = labeled_comprehension(\n",
    "                    current_channel_image,\n",
    "                    cell_mask,\n",
    "                    cell_ids,\n",
    "                    lambda pixels: np.percentile(pixels, 95) if len(pixels) > 0 else np.nan,\n",
    "                    float,\n",
    "                    np.nan\n",
    "                )\n",
    "                df_intensity_marker[f'{marker_name_to_quantify}_p95'] = p95_values\n",
    "            else:\n",
    "                progress_bar.set_postfix({\"Marker\": marker_name_to_quantify[:20], \"Type\": \"mean-only\"})\n",
    "            \n",
    "            # Merge mit Haupttabelle\n",
    "            if df_features.empty and not df_intensity_marker.empty: # Wenn df_features leer war, aber jetzt Intensit\u00c3\u00a4ten da sind\n",
    "                df_features = df_intensity_marker\n",
    "            elif not df_features.empty and not df_intensity_marker.empty :\n",
    "                 df_features = pd.merge(df_features, df_intensity_marker, on='CellID', how='left')\n",
    "            elif not df_intensity_marker.empty : # Sollte nicht passieren, wenn df_features leer und Maske leer war\n",
    "                 print(f\"WARNUNG: df_features ist leer, aber Intensit\u00c3\u00a4tsdaten f\u00c3\u00bcr {marker_name_to_quantify} vorhanden. Merging nicht m\u00c3\u00b6glich.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"    WARNUNG: Kanalindex {channel_index} f\u00c3\u00bcr Marker '{marker_name_to_quantify}' ist au\u00c3\u0178erhalb der Bilddimensionen \"\n",
    "                  f\"(max Index ist {full_image_data_hwc.shape[2]-1}). \u00c3\u0153berspringe Intensit\u00c3\u00a4tsmessung.\")\n",
    "            if not df_features.empty: df_features[marker_name_to_quantify] = np.nan\n",
    "    \n",
    "    if compute_p95:\n",
    "        print(f\"  \u00e2\u0153\u201c {membrane_count} Membranmarker bekamen zus\u00c3\u00a4tzlich _p95 Spalten\")\n",
    "\n",
    "# --- WICHTIG: Stabilisiere ALLE Intensit\u00c3\u00a4tswerte (Cylinter macht log-Transformationen) ---\n",
    "print(\"\\n--- Stabilisiere Intensit\u00c3\u00a4tswerte (ersetze \u00e2\u2030\u00a40 mit Minimum) ---\")\n",
    "intensity_cols = [\n",
    "    col for col in df_features.columns\n",
    "    if isinstance(col, str) and ('_c' in col or 'intensity' in col.lower())\n",
    "]\n",
    "\n",
    "stabilized_count = 0\n",
    "for col in intensity_cols:\n",
    "    series = df_features[col].astype(float)\n",
    "    \n",
    "    # Finde Minimalwert aller positiven Werte\n",
    "    positive = series[series > 0]\n",
    "    if not positive.empty:\n",
    "        floor = positive.min()\n",
    "    else:\n",
    "        floor = 1.0\n",
    "    \n",
    "    # Ersetze alle Null-/Negativwerte\n",
    "    replaced_mask = series <= 0\n",
    "    if replaced_mask.any():\n",
    "        df_features.loc[replaced_mask, col] = floor\n",
    "        stabilized_count += 1\n",
    "        print(f\"  \u00e2\u0153\u201c {col}: {replaced_mask.sum()} Werte ersetzt durch {floor:.3g}\")\n",
    "    \n",
    "    # Zus\u00c3\u00a4tzlich: Ersetze NaN/inf Werte\n",
    "    invalid_mask = ~np.isfinite(df_features[col])\n",
    "    if invalid_mask.any():\n",
    "        df_features.loc[invalid_mask, col] = floor\n",
    "        print(f\"  \u00e2\u0153\u201c {col}: {invalid_mask.sum()} NaN/inf Werte ersetzt\")\n",
    "\n",
    "if stabilized_count == 0:\n",
    "    print(\"  \u00e2\u0153\u201c Alle Intensit\u00c3\u00a4tswerte bereits positiv (keine Stabilisierung n\u00c3\u00b6tig)\")\n",
    "else:\n",
    "    print(f\"  \u00e2\u0153\u201c {stabilized_count} Spalten stabilisiert\")\n",
    "print(\"--- Stabilisierung abgeschlossen ---\\n\")\n",
    "\n",
    "# --- Sicherstellen, dass der Zielordner f\u00c3\u00bcr die CSV existiert ---\n",
    "if 'path_csv_output_for_quant' not in locals(): # Sollte durch Zelle 2 definiert sein\n",
    "     raise NameError(\"Variable 'path_csv_output_for_quant' ist nicht definiert.\")\n",
    "\n",
    "csv_output_directory = path_csv_output_for_quant.parent\n",
    "try:\n",
    "    csv_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"INFO: Zielordner f\u00c3\u00bcr CSV sichergestellt/erstellt: {csv_output_directory}\")\n",
    "except Exception as e_mkdir:\n",
    "    print(f\"FEHLER beim Erstellen des CSV-Zielordners {csv_output_directory}: {e_mkdir}\")\n",
    "    raise\n",
    "# --- Ende Ordnererstellung ---\n",
    "\n",
    "# Sicherstellen, dass 'CellID' die erste Spalte ist, falls sie existiert und df nicht leer ist\n",
    "if not df_features.empty and 'CellID' in df_features.columns:\n",
    "    cols = ['CellID'] + [col for col in df_features.columns if col != 'CellID']\n",
    "    df_features = df_features[cols]\n",
    "elif df_features.empty:\n",
    "    print(\"INFO: df_features ist leer. Es wird eine leere CSV-Datei gespeichert.\")\n",
    "else: # df_features nicht leer, aber 'CellID' fehlt\n",
    "    print(\"WARNUNG: Spalte 'CellID' fehlt im finalen DataFrame, obwohl Daten vorhanden sind.\")\n",
    "\n",
    "\n",
    "print(f\"Erste Zeilen der Feature-Tabelle ({df_features.shape}):\\n{df_features.head()}\")\n",
    "df_features.to_csv(path_csv_output_for_quant, index=False)\n",
    "print(f\"Einzelzell-Feature-Tabelle gespeichert unter: {path_csv_output_for_quant}\")\n",
    "print(\"Zelle 2.5 (Manuelle Quantifizierung) erfolgreich ausgef\u00c3\u00bchrt.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 2.75: PRE-FLIGHT CHECK - \u00c3\u0153berpr\u00c3\u00bcfe alle Bedingungen vor CyLinter Start\n",
    "print(\"=\" * 80)\n",
    "print(\"PRE-FLIGHT CHECK: \u00c3\u0153berpr\u00c3\u00bcfe alle Voraussetzungen f\u00c3\u00bcr CyLinter\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Z\u00c3\u00a4hler f\u00c3\u00bcr Erfolge und Fehler\n",
    "checks_passed = 0\n",
    "checks_failed = 0\n",
    "warnings = 0\n",
    "\n",
    "# --- CHECK 1: Ben\u00c3\u00b6tigte Variablen ---\n",
    "print(\"\\n[1] \u00c3\u0153berpr\u00c3\u00bcfe ben\u00c3\u00b6tigte Variablen aus vorherigen Zellen...\")\n",
    "required_vars = ['BASE_DIR', 'CONFIG_PATH', 'MARKERS_CSV_PATH', 'all_channel_names_from_markers_csv', 'SAMPLE_NAME_FOR_FILES_QUANT']\n",
    "for var_name in required_vars:\n",
    "    if var_name in locals() or var_name in globals():\n",
    "        print(f\"  \u00e2\u0153\u201c {var_name} vorhanden\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"  \u00e2\u0153\u2014 FEHLER: {var_name} nicht gefunden! F\u00c3\u00bchren Sie vorherige Zellen aus.\")\n",
    "        checks_failed += 1\n",
    "\n",
    "# --- CHECK 2: markers.csv Format ---\n",
    "print(\"\\n[2] \u00c3\u0153berpr\u00c3\u00bcfe markers.csv Format...\")\n",
    "try:\n",
    "    df_markers_check = pd.read_csv(MARKERS_CSV_PATH)\n",
    "    print(f\"  \u00e2\u0153\u201c markers.csv geladen: {len(df_markers_check)} Eintr\u00c3\u00a4ge\")\n",
    "    checks_passed += 1\n",
    "    \n",
    "    # WICHTIG: Pr\u00c3\u00bcfe dass markers.csv _cX Suffixe hat (f\u00c3\u00bcr aggregateData)\n",
    "    sample_markers = df_markers_check['marker_name'].head(5).tolist()\n",
    "    has_cycle_suffix = any('_c' in str(m) for m in sample_markers)\n",
    "    \n",
    "    if not has_cycle_suffix:\n",
    "        print(f\"  \u00e2\u0153\u2014 FEHLER: Marker-Namen haben KEINE _cX Suffix: {sample_markers[:3]}\")\n",
    "        print(f\"     \u00e2\u2020\u2019 markers.csv sollte _cX haben (f\u00c3\u00bcr aggregateData CSV-Matching)\")\n",
    "        print(f\"     \u00e2\u2020\u2019 Zelle 1 sollte dies automatisch korrigieren\")\n",
    "        checks_failed += 1\n",
    "    else:\n",
    "        print(f\"  \u00e2\u0153\u201c Marker-Namen haben _cX (korrekt f\u00c3\u00bcr aggregateData): {sample_markers[:3]}\")\n",
    "        checks_passed += 1\n",
    "    \n",
    "    # Pr\u00c3\u00bcfe auf Leerzeichen\n",
    "    has_spaces = any(' ' in str(m) for m in df_markers_check['marker_name'])\n",
    "    if has_spaces:\n",
    "        print(f\"  \u00e2\u0161\u00a0 WARNUNG: Leerzeichen in Marker-Namen gefunden\")\n",
    "        print(f\"     \u00e2\u2020\u2019 Zelle 1 sollte diese entfernen\")\n",
    "        warnings += 1\n",
    "    else:\n",
    "        print(f\"  \u00e2\u0153\u201c Keine Leerzeichen in Marker-Namen\")\n",
    "        checks_passed += 1\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  \u00e2\u0153\u2014 FEHLER beim Laden: {e}\")\n",
    "    checks_failed += 1\n",
    "\n",
    "# --- CHECK 3: cylinter_config.yml ---\n",
    "print(\"\\n[3] \u00c3\u0153berpr\u00c3\u00bcfe cylinter_config.yml...\")\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"  \u00e2\u0153\u201c Config geladen\")\n",
    "    checks_passed += 1\n",
    "    \n",
    "    # Pr\u00c3\u00bcfe sampleMetadata Format\n",
    "    smd = config.get('sampleMetadata', {})\n",
    "    if smd:\n",
    "        sample_key = list(smd.keys())[0]\n",
    "        sample_value = smd[sample_key]\n",
    "        if isinstance(sample_value, list) and len(sample_value) == 5:\n",
    "            print(f\"  \u00e2\u0153\u201c sampleMetadata korrekt: {sample_key} -> 5-Element Liste\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  \u00e2\u0153\u2014 FEHLER: sampleMetadata Format falsch: {sample_value}\")\n",
    "            print(f\"     \u00e2\u2020\u2019 Muss 5-Element Liste sein: [name, condition, replicate, group, order]\")\n",
    "            checks_failed += 1\n",
    "    else:\n",
    "        print(f\"  \u00e2\u0153\u2014 FEHLER: sampleMetadata fehlt\")\n",
    "        checks_failed += 1\n",
    "    \n",
    "    # Pr\u00c3\u00bcfe counterstainChannel\n",
    "    counterstain = config.get('counterstainChannel')\n",
    "    if counterstain:\n",
    "        if counterstain in df_markers_check['marker_name'].values:\n",
    "            print(f\"  \u00e2\u0153\u201c counterstainChannel '{counterstain}' in markers.csv gefunden\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  \u00e2\u0153\u2014 FEHLER: counterstainChannel '{counterstain}' NICHT in markers.csv!\")\n",
    "            print(f\"     \u00e2\u2020\u2019 Verf\u00c3\u00bcgbare DAPI Marker: {[m for m in df_markers_check['marker_name'] if 'DAPI' in str(m)][:3]}\")\n",
    "            checks_failed += 1\n",
    "    else:\n",
    "        print(f\"  \u00e2\u0153\u2014 FEHLER: counterstainChannel fehlt in Config\")\n",
    "        checks_failed += 1\n",
    "    \n",
    "    # Pr\u00c3\u00bcfe samplesForROISelection\n",
    "    roi_samples = config.get('samplesForROISelection', [])\n",
    "    if roi_samples:\n",
    "        roi_sample_name = roi_samples[0] if roi_samples else None\n",
    "        if roi_sample_name == sample_key:\n",
    "            print(f\"  \u00e2\u0153\u201c samplesForROISelection stimmt mit sampleMetadata \u00c3\u00bcberein: '{roi_sample_name}'\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  \u00e2\u0153\u2014 FEHLER: samplesForROISelection '{roi_sample_name}' != sampleMetadata '{sample_key}'\")\n",
    "            checks_failed += 1\n",
    "    else:\n",
    "        print(f\"  \u00e2\u0161\u00a0 WARNUNG: samplesForROISelection ist leer\")\n",
    "        warnings += 1\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  \u00e2\u0153\u2014 FEHLER beim Laden: {e}\")\n",
    "    checks_failed += 1\n",
    "\n",
    "# --- CHECK 4: CSV Datei ---\n",
    "print(\"\\n[4] \u00c3\u0153berpr\u00c3\u00bcfe erstellte CSV-Datei...\")\n",
    "try:\n",
    "    csv_path = BASE_DIR / 'csv' / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.csv\"\n",
    "    if csv_path.exists():\n",
    "        df_csv = pd.read_csv(csv_path)\n",
    "        print(f\"  \u00e2\u0153\u201c CSV gefunden: {csv_path.name}\")\n",
    "        print(f\"    Zeilen: {len(df_csv)}, Spalten: {len(df_csv.columns)}\")\n",
    "        checks_passed += 1\n",
    "        \n",
    "        # Pr\u00c3\u00bcfe CellID\n",
    "        if 'CellID' in df_csv.columns:\n",
    "            print(f\"  \u00e2\u0153\u201c CellID Spalte vorhanden\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  \u00e2\u0153\u2014 FEHLER: CellID Spalte fehlt!\")\n",
    "            checks_failed += 1\n",
    "        \n",
    "        # KORRIGIERT: Vergleiche nur mit NICHT-EXCLUDIERTEN Markern!\n",
    "        csv_marker_cols = [col for col in df_csv.columns if col not in ['CellID', 'Area', 'X_centroid', 'Y_centroid', \n",
    "                                                                          'MajorAxisLength', 'MinorAxisLength', \n",
    "                                                                          'Eccentricity', 'Solidity', 'Orientation']]\n",
    "        \n",
    "        # Hole excludierte Marker aus Config\n",
    "        excluded_markers = config.get('markersToExclude', [])\n",
    "        expected_markers = set(df_markers_check['marker_name'].values) - set(excluded_markers)\n",
    "        csv_marker_set = set(csv_marker_cols)\n",
    "        \n",
    "        if csv_marker_set == expected_markers:\n",
    "            print(f\"  \u00e2\u0153\u201c CSV Spalten stimmen mit erwarteten Markern \u00c3\u00bcberein ({len(csv_marker_set)} Marker)\")\n",
    "            print(f\"    ({len(excluded_markers)} Marker ausgeschlossen wie konfiguriert)\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            missing_in_csv = expected_markers - csv_marker_set\n",
    "            extra_in_csv = csv_marker_set - expected_markers\n",
    "            if missing_in_csv:\n",
    "                print(f\"  \u00e2\u0153\u2014 FEHLER: {len(missing_in_csv)} erwartete Marker fehlen in CSV\")\n",
    "                print(f\"     Erste 3: {list(missing_in_csv)[:3]}\")\n",
    "                checks_failed += 1\n",
    "            if extra_in_csv:\n",
    "                print(f\"  \u00e2\u0161\u00a0 WARNUNG: {len(extra_in_csv)} unerwartete Marker in CSV\")\n",
    "                print(f\"     Erste 3: {list(extra_in_csv)[:3]}\")\n",
    "                warnings += 1\n",
    "            if not missing_in_csv and not extra_in_csv:\n",
    "                checks_passed += 1\n",
    "                \n",
    "    else:\n",
    "        print(f\"  \u00e2\u0153\u2014 FEHLER: CSV nicht gefunden: {csv_path}\")\n",
    "        print(f\"     \u00e2\u2020\u2019 F\u00c3\u00bchren Sie Zelle 2.5 aus\")\n",
    "        checks_failed += 1\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  \u00e2\u0153\u2014 FEHLER: {e}\")\n",
    "    checks_failed += 1\n",
    "\n",
    "# --- CHECK 5: Dateiz\u00c3\u00a4hlung (KORRIGIERT: ignoriere .DS_Store) ---\n",
    "print(\"\\n[5] \u00c3\u0153berpr\u00c3\u00bcfe Dateiz\u00c3\u00a4hlung in Ordnern...\")\n",
    "folders_to_check = ['csv', 'tif', 'seg', 'mask']\n",
    "file_counts = {}\n",
    "for folder in folders_to_check:\n",
    "    folder_path = BASE_DIR / folder\n",
    "    if folder_path.exists():\n",
    "        # Ignoriere .DS_Store und andere versteckte Dateien\n",
    "        files = [f for f in folder_path.glob('*') if f.is_file() and not f.name.startswith('.')]\n",
    "        file_counts[folder] = len(files)\n",
    "    else:\n",
    "        file_counts[folder] = 0\n",
    "\n",
    "all_counts_equal = len(set(file_counts.values())) == 1\n",
    "if all_counts_equal and file_counts['csv'] > 0:\n",
    "    print(f\"  \u00e2\u0153\u201c Alle Ordner haben gleiche Anzahl Dateien: {file_counts}\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"  \u00e2\u0153\u2014 FEHLER: Unterschiedliche Dateianzahl: {file_counts}\")\n",
    "    print(f\"     \u00e2\u2020\u2019 Alle Ordner m\u00c3\u00bcssen GENAU 1 Datei haben mit gleichem Namen\")\n",
    "    checks_failed += 1\n",
    "\n",
    "# --- ZUSAMMENFASSUNG ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ZUSAMMENFASSUNG PRE-FLIGHT CHECK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\u00e2\u0153\u201c Erfolgreich: {checks_passed}\")\n",
    "print(f\"\u00e2\u0153\u2014 Fehler:      {checks_failed}\")\n",
    "print(f\"\u00e2\u0161\u00a0 Warnungen:   {warnings}\")\n",
    "print()\n",
    "\n",
    "if checks_failed == 0:\n",
    "    print(\"\u00f0\u0178\u017d\u2030 ALLE CHECKS BESTANDEN! CyLinter kann gestartet werden (Zelle 8).\")\n",
    "    print()\n",
    "    print(\"N\u00c3\u201eCHSTE SCHRITTE:\")\n",
    "    print(\"  1. F\u00c3\u00bchren Sie Zelle 8 aus (Konfigurations-GUI)\")\n",
    "    print(\"  2. F\u00c3\u00bchren Sie Zelle 9 aus (Pipeline-Start)\")\n",
    "else:\n",
    "    print(\"\u00e2\u009d\u0152 FEHLER GEFUNDEN! Bitte beheben Sie die Probleme vor CyLinter Start.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Pre-Flight Check abgeschlossen.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a7525",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ?? Zelle 8: Konfigurations-GUI (Tag 1)\n",
    "\n",
    "Marker-Subset ausw?hlen (Schnellauswahl oder Custom). Speichert nur `markersToExclude` in `cylinter_config.yml`; Pipeline startet in Zelle 9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8db0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 8: Konfigurations-GUI (TAG 1 - Erste Analyse)\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"KONFIGURATIONS-GUI (TAG 1 - ERSTE ANALYSE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# \u00c3\u0153berpr\u00c3\u00bcfe ben\u00c3\u00b6tigte Variablen\n",
    "if 'BASE_DIR' not in locals():\n",
    "    BASE_DIR = Path.cwd()\n",
    "if 'CONFIG_PATH' not in locals():\n",
    "    CONFIG_PATH = BASE_DIR / \"cylinter_config.yml\"\n",
    "if 'MARKERS_CSV_PATH' not in locals():\n",
    "    MARKERS_CSV_PATH = BASE_DIR / \"markers.csv\"\n",
    "\n",
    "# Lade markers.csv\n",
    "df_markers = pd.read_csv(MARKERS_CSV_PATH)\n",
    "\n",
    "# Kategorisiere Marker\n",
    "dapi_markers = df_markers[df_markers['marker_name'].str.contains('DAPI', case=False, na=False)]['marker_name'].tolist()\n",
    "af_markers = df_markers[df_markers['marker_name'].str.contains('AF[12]_', case=True, na=False, regex=True)]['marker_name'].tolist()\n",
    "bio_markers = df_markers[\n",
    "    ~df_markers['marker_name'].str.contains('DAPI', case=False, na=False) &\n",
    "    ~df_markers['marker_name'].str.contains('AF[12]_', case=True, na=False, regex=True)\n",
    "]['marker_name'].tolist()\n",
    "\n",
    "print(f\"Marker geladen: {len(dapi_markers)} DAPI, {len(af_markers)} AF, {len(bio_markers)} Bio\")\n",
    "\n",
    "# Quick Options\n",
    "quick_label = widgets.HTML(\"<h4>Schnellauswahl:</h4>\")\n",
    "quick_option = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('Alle 96 Marker verwenden', 'all'),\n",
    "        ('Alle ausser AF (58 Marker: DAPI + Bio)', 'no_af'),\n",
    "        ('Nur Bio-Marker (ohne DAPI/AF)', 'bio_only'),\n",
    "        ('Custom (unten auswaehlen)', 'custom')\n",
    "    ],\n",
    "    value='no_af',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "quick_info = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #e3f2fd; padding: 10px; border-left: 4px solid #2196F3; margin: 10px 0;\">\n",
    "<b>Empfehlung:</b> \"Alle ausser AF\" (58 Marker)<br>\n",
    "Autofluoreszenz-Marker werden automatisch ausgeschlossen.\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Bio-Marker Checkboxen\n",
    "bio_checkboxes = {}\n",
    "bio_checkbox_widgets = []\n",
    "\n",
    "for marker in sorted(bio_markers):\n",
    "    cb = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description=marker,\n",
    "        indent=False,\n",
    "        layout=widgets.Layout(width='220px'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    bio_checkboxes[marker] = cb\n",
    "    bio_checkbox_widgets.append(cb)\n",
    "\n",
    "bio_grid = widgets.GridBox(bio_checkbox_widgets, layout=widgets.Layout(\n",
    "    grid_template_columns='repeat(3, 220px)',\n",
    "    grid_gap='5px',\n",
    "    margin='10px 0',\n",
    "    max_height='400px',\n",
    "    overflow_y='auto'\n",
    "))\n",
    "\n",
    "# Bio-Marker Controls\n",
    "bio_select_all = widgets.Button(description='Alle', layout=widgets.Layout(width='120px'))\n",
    "bio_deselect_all = widgets.Button(description='Keine', layout=widgets.Layout(width='120px'))\n",
    "\n",
    "def bio_select_all_click(_):\n",
    "    for cb in bio_checkboxes.values():\n",
    "        cb.value = True\n",
    "\n",
    "def bio_deselect_all_click(_):\n",
    "    for cb in bio_checkboxes.values():\n",
    "        cb.value = False\n",
    "\n",
    "bio_select_all.on_click(bio_select_all_click)\n",
    "bio_deselect_all.on_click(bio_deselect_all_click)\n",
    "\n",
    "bio_controls = widgets.HBox([bio_select_all, bio_deselect_all])\n",
    "detail_label = widgets.HTML(\"<h4>Custom: Bio-Marker einzeln auswaehlen</h4>\")\n",
    "\n",
    "# Status Output\n",
    "status_output = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ccc', padding='10px', margin='10px 0',\n",
    "    max_height='300px', overflow_y='auto'\n",
    "))\n",
    "\n",
    "# Save Function\n",
    "def save_and_ready(b):\n",
    "    \"\"\"Speichert nur Marker-Auswahl - Pipeline wird in Zelle 9 gestartet\"\"\"\n",
    "    with status_output:\n",
    "        status_output.clear_output()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SPEICHERE MARKER-KONFIGURATION\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Marker-Auswahl\n",
    "        option = quick_option.value\n",
    "        markers_to_exclude = []\n",
    "        \n",
    "        if option == 'all':\n",
    "            markers_to_exclude = []\n",
    "            marker_desc = \"Alle 96 Marker\"\n",
    "        elif option == 'no_af':\n",
    "            markers_to_exclude = af_markers\n",
    "            marker_desc = f\"58 Marker (DAPI + Bio, ohne {len(af_markers)} AF)\"\n",
    "        elif option == 'bio_only':\n",
    "            markers_to_exclude = dapi_markers + af_markers\n",
    "            marker_desc = f\"{len(bio_markers)} Bio-Marker (ohne DAPI/AF)\"\n",
    "        elif option == 'custom':\n",
    "            excluded_bio = [m for m, cb in bio_checkboxes.items() if not cb.value]\n",
    "            markers_to_exclude = af_markers + excluded_bio\n",
    "            marker_desc = f\"{len(dapi_markers) + len(bio_markers) - len(excluded_bio)} Marker (Custom)\"\n",
    "        \n",
    "        print(f\"Marker-Auswahl: {marker_desc}\")\n",
    "        print(f\"   Ausgeschlossen: {len(markers_to_exclude)} Marker\")\n",
    "        print(f\"   Werden verwendet: {96 - len(markers_to_exclude)} Marker\")\n",
    "        \n",
    "        # Schreibe Config\n",
    "        try:\n",
    "            # Backup\n",
    "            backup_path = CONFIG_PATH.with_suffix('.yml.backup')\n",
    "            import shutil\n",
    "            shutil.copy2(CONFIG_PATH, backup_path)\n",
    "            print(f\"\\nBackup erstellt: {backup_path.name}\")\n",
    "            \n",
    "            # Lade Config\n",
    "            with open(CONFIG_PATH, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            \n",
    "            # Update markersToExclude\n",
    "            config['markersToExclude'] = markers_to_exclude\n",
    "            \n",
    "            # Schreibe Config\n",
    "            temp_path = CONFIG_PATH.with_suffix('.yml.tmp')\n",
    "            with open(temp_path, 'w') as f:\n",
    "                yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "            temp_path.replace(CONFIG_PATH)\n",
    "            \n",
    "            print(f\"cylinter_config.yml aktualisiert\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"KONFIGURATION GESPEICHERT!\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "            \n",
    "            print(\"NAECHSTER SCHRITT:\")\n",
    "            print(\"   Fuehren Sie ZELLE 9 aus, um die Pipeline zu starten!\")\n",
    "            print(\"   Die Pipeline laeuft von aggregateData bis zum Ende durch.\")\n",
    "            print(\"   Module mit existierenden Checkpoints werden uebersprungen.\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nFEHLER: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "save_btn = widgets.Button(\n",
    "    description='Speichern & Bereit',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='220px', height='45px'),\n",
    "    icon='check',\n",
    "    tooltip='Speichert Marker-Konfiguration (startet KEINE Pipeline!)'\n",
    ")\n",
    "save_btn.on_click(save_and_ready)\n",
    "\n",
    "# UI\n",
    "info_box = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 15px; border-left: 4px solid #ffc107; margin-bottom: 15px;\">\n",
    "<b>Funktionsweise:</b><br>\n",
    "Diese GUI konfiguriert nur die <b>Marker-Auswahl</b><br>\n",
    "Die Pipeline startet in <b>Zelle 9</b><br>\n",
    "CyLinter laeuft automatisch durch alle Module von <b>aggregateData bis zum Ende</b><br>\n",
    "Module mit existierenden Checkpoints werden <b>automatisch uebersprungen</b><br><br>\n",
    "<b>Fuer TAG 1 (erste Analyse):</b> Waehlen Sie Marker, Speichern, Zelle 9 ausfuehren<br>\n",
    "<b>Fuer TAG 2+ (neue Marker):</b> Verwenden Sie Zelle 13 (Erweiterte Steuerung)\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Marker-Konfiguration</h3>\"),\n",
    "    info_box,\n",
    "    quick_label,\n",
    "    quick_option,\n",
    "    quick_info,\n",
    "    detail_label,\n",
    "    bio_controls,\n",
    "    bio_grid,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    save_btn,\n",
    "    status_output\n",
    "])\n",
    "\n",
    "display(ui)\n",
    "\n",
    "print(\"\\nKonfigurations-GUI bereit!\")\n",
    "print(\"   Waehlen Sie Marker-Strategie und klicken Sie 'Speichern & Bereit'\")\n",
    "print(\"   Pipeline wird dann in Zelle 9 gestartet.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 9: Pipeline-Start (TAG 1) - MIT INTELLIGENTER CHECKPOINT-ERKENNUNG\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CYLINTER PIPELINE-START (TAG 1)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Intelligente Checkpoint-Erkennung: Startet bei erstem fehlenden Modul!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ueberpruefe benoetigte Variablen\n",
    "if 'BASE_DIR' not in locals():\n",
    "    BASE_DIR = Path.cwd()\n",
    "if 'CONFIG_PATH' not in locals():\n",
    "    CONFIG_PATH = BASE_DIR / \"cylinter_config.yml\"\n",
    "if 'CYLINTER_ENV_PATH' not in locals():\n",
    "    CYLINTER_ENV_PATH = Path(\"/opt/anaconda3/envs/cylinter_ben\")\n",
    "if 'CHECKPOINT_DIR' not in locals():\n",
    "    CHECKPOINT_DIR = BASE_DIR / \"cylinter_output_prune_test\" / \"checkpoints\"\n",
    "\n",
    "# CyLinter Executables\n",
    "cylinter_executable = CYLINTER_ENV_PATH / \"bin\" / \"cylinter\"\n",
    "python_executable = CYLINTER_ENV_PATH / \"bin\" / \"python\"\n",
    "\n",
    "# Alle Module in Pipeline-Reihenfolge\n",
    "ALL_MODULES = [\n",
    "    \"aggregateData\", \"selectROIs\", \"intensityFilter\", \"areaFilter\",\n",
    "    \"cycleCorrelation\", \"logTransform\", \"pruneOutliers\", \"metaQC\", \"PCA\",\n",
    "    \"setContrast\", \"gating\", \"clustering\", \"clustermap\", \"frequencyStats\", \"curateThumbnails\"\n",
    "]\n",
    "\n",
    "# Output Widget\n",
    "output_widget = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ccc', padding='10px', margin='10px 0',\n",
    "    max_height='600px', overflow_y='auto'\n",
    "))\n",
    "\n",
    "# Pruefe ob Config existiert\n",
    "if not CONFIG_PATH.exists():\n",
    "    error_box = widgets.HTML(\"\"\"\n",
    "    <div style=\"background-color: #ffebee; padding: 20px; border-left: 4px solid #f44336;\">\n",
    "    <h3>FEHLER: Konfiguration fehlt!</h3>\n",
    "    <p>Die Datei <code>cylinter_config.yml</code> wurde nicht gefunden.</p>\n",
    "    <p><b>Bitte fuehren Sie zuerst ZELLE 8 aus!</b></p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    ui = widgets.VBox([error_box])\n",
    "    display(ui)\n",
    "    \n",
    "else:\n",
    "    # Config vorhanden - analysiere Checkpoints\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    markers_excluded = len(config.get('markersToExclude', []))\n",
    "    \n",
    "    # INTELLIGENTE CHECKPOINT-ERKENNUNG\n",
    "    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    existing_checkpoints = []\n",
    "    missing_checkpoints = []\n",
    "    \n",
    "    for module in ALL_MODULES:\n",
    "        checkpoint_file = CHECKPOINT_DIR / f\"{module}.parquet\"\n",
    "        if checkpoint_file.exists():\n",
    "            existing_checkpoints.append(module)\n",
    "        else:\n",
    "            missing_checkpoints.append(module)\n",
    "    \n",
    "    # Bestimme Startmodul\n",
    "    if len(missing_checkpoints) == 0:\n",
    "        suggested_start = None\n",
    "        status_message = \"ALLE CHECKPOINTS VORHANDEN! Pipeline komplett.\"\n",
    "        status_color = \"#4caf50\"\n",
    "    else:\n",
    "        suggested_start = missing_checkpoints[0]\n",
    "        status_message = f\"Erstes fehlendes Checkpoint: {suggested_start}\"\n",
    "        status_color = \"#ff9800\"\n",
    "    \n",
    "    # Zeige Status\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CHECKPOINT-ANALYSE:\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    print(f\"Existierende Checkpoints: {len(existing_checkpoints)}/15\")\n",
    "    for cp in existing_checkpoints:\n",
    "        print(f\"   [OK] {cp}\")\n",
    "    print(f\"\\nFehlende Checkpoints: {len(missing_checkpoints)}/15\")\n",
    "    for cp in missing_checkpoints:\n",
    "        print(f\"   [--] {cp}\")\n",
    "    print()\n",
    "    \n",
    "    # Pipeline-Start-Funktion\n",
    "    def start_pipeline(b):\n",
    "        \"\"\"Startet die Pipeline beim ersten fehlenden Modul\"\"\"\n",
    "        with output_widget:\n",
    "            output_widget.clear_output()\n",
    "            \n",
    "            # Re-check Checkpoints (falls sich was geaendert hat)\n",
    "            missing = []\n",
    "            for module in ALL_MODULES:\n",
    "                if not (CHECKPOINT_DIR / f\"{module}.parquet\").exists():\n",
    "                    missing.append(module)\n",
    "            \n",
    "            if len(missing) == 0:\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"ALLE CHECKPOINTS VORHANDEN!\")\n",
    "                print(\"=\"*70 + \"\\n\")\n",
    "                print(\"Die Pipeline ist komplett durchgelaufen.\")\n",
    "                print(\"Falls Sie Module neu berechnen wollen, nutzen Sie Zelle 12.\")\n",
    "                return\n",
    "            \n",
    "            start_module = missing[0]\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(f\"STARTE PIPELINE BEI: {start_module}\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "            print(\"Pipeline-Reihenfolge:\")\n",
    "            for i, mod in enumerate(ALL_MODULES, 1):\n",
    "                status = \"[SKIP]\" if mod in existing_checkpoints else \"[RUN!]\"\n",
    "                marker = \">>>\" if mod == start_module else \"   \"\n",
    "                print(f\"{marker} {i:2}. {status} {mod}\")\n",
    "            print()\n",
    "            \n",
    "            # WICHTIG: Wenn start_module NICHT aggregateData ist, aber auch nicht\n",
    "            # das vorherige Modul hat, muessen wir bei aggregateData starten!\n",
    "            if start_module != \"aggregateData\":\n",
    "                module_index = ALL_MODULES.index(start_module)\n",
    "                previous_module = ALL_MODULES[module_index - 1]\n",
    "                previous_checkpoint = CHECKPOINT_DIR / f\"{previous_module}.parquet\"\n",
    "                \n",
    "                if not previous_checkpoint.exists():\n",
    "                    print(f\"[!] WARNUNG: Vorheriges Checkpoint '{previous_module}.parquet' fehlt!\")\n",
    "                    print(f\"[!] Muss bei 'aggregateData' starten statt '{start_module}'!\\n\")\n",
    "                    start_module = \"aggregateData\"\n",
    "            \n",
    "            if cylinter_executable.exists():\n",
    "                cmd = [str(cylinter_executable), \"--module\", start_module, str(CONFIG_PATH)]\n",
    "                print(f\"[*] Command: cylinter --module {start_module} config.yml\")\n",
    "            else:\n",
    "                cmd = [str(python_executable), \"-m\", \"cylinter.cylinter\", \"--module\", start_module, str(CONFIG_PATH)]\n",
    "                print(f\"[*] Command: python -m cylinter.cylinter --module {start_module} config.yml\")\n",
    "            \n",
    "            print(f\"\\n[!] Pipeline laeuft von '{start_module}' bis zum Ende!\")\n",
    "            print(\"[!] Interaktive Module (selectROIs, setContrast, gating) oeffnen GUIs\\n\")\n",
    "            \n",
    "            overall_start = time.time()\n",
    "            \n",
    "            try:\n",
    "                env = os.environ.copy()\n",
    "                process = subprocess.Popen(\n",
    "                    cmd, cwd=str(BASE_DIR), env=env,\n",
    "                    stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                    universal_newlines=True, bufsize=1\n",
    "                )\n",
    "                \n",
    "                # Stream Output\n",
    "                for line in process.stdout:\n",
    "                    print(line, end='')\n",
    "                \n",
    "                return_code = process.wait()\n",
    "                total_duration = time.time() - overall_start\n",
    "                \n",
    "                # Zusammenfassung\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                if return_code == 0:\n",
    "                    print(\"PIPELINE ERFOLGREICH!\")\n",
    "                    print(f\"   Laufzeit: {total_duration:.1f}s ({total_duration/60:.1f} Min)\")\n",
    "                else:\n",
    "                    print(f\"PIPELINE FEHLER (Exit Code: {return_code})\")\n",
    "                    print(f\"   Laufzeit: {total_duration:.1f}s ({total_duration/60:.1f} Min)\")\n",
    "                print(\"=\"*70 + \"\\n\")\n",
    "                \n",
    "                # Zeige welche Checkpoints jetzt existieren\n",
    "                print(\"Checkpoint-Status nach Run:\")\n",
    "                for module in ALL_MODULES:\n",
    "                    cp_file = CHECKPOINT_DIR / f\"{module}.parquet\"\n",
    "                    status = \"[OK]\" if cp_file.exists() else \"[--]\"\n",
    "                    print(f\"   {status} {module}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\n[!] FEHLER beim Pipeline-Start: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Start Button\n",
    "    start_btn = widgets.Button(\n",
    "        description='Pipeline starten',\n",
    "        button_style='success' if suggested_start else 'primary',\n",
    "        layout=widgets.Layout(width='200px', height='50px'),\n",
    "        icon='play',\n",
    "        tooltip=f'Startet bei: {suggested_start}' if suggested_start else 'Pipeline komplett'\n",
    "    )\n",
    "    start_btn.on_click(start_pipeline)\n",
    "    \n",
    "    # Info Box\n",
    "    info_html = f\"\"\"\n",
    "    <div style=\"background-color: {status_color}22; padding: 15px; border-left: 4px solid {status_color};\">\n",
    "    <b>Status:</b> {status_message}<br><br>\n",
    "    <b>Konfiguration:</b><br>\n",
    "    {96 - markers_excluded} Marker werden verwendet<br><br>\n",
    "    <b>Existierende Checkpoints:</b> {len(existing_checkpoints)}/15<br>\n",
    "    <b>Fehlende Checkpoints:</b> {len(missing_checkpoints)}/15<br><br>\n",
    "    \"\"\"\n",
    "    \n",
    "    if suggested_start:\n",
    "        info_html += f\"\"\"\n",
    "        <b>Naechster Schritt:</b><br>\n",
    "        Pipeline wird bei <b>{suggested_start}</b> starten<br>\n",
    "        und bis zum Ende durchlaufen.<br><br>\n",
    "        \"\"\"\n",
    "    else:\n",
    "        info_html += \"\"\"\n",
    "        <b>Pipeline komplett!</b><br>\n",
    "        Alle Module wurden bereits ausgefuehrt.<br><br>\n",
    "        \"\"\"\n",
    "    \n",
    "    info_html += \"\"\"\n",
    "    <b>WICHTIG:</b><br>\n",
    "    CyLinter ueberschreibt alle Checkpoints ab dem Startmodul!<br>\n",
    "    Wenn Sie einzelne Module neu berechnen wollen, nutzen Sie Zelle 12.\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    info_box = widgets.HTML(info_html)\n",
    "    \n",
    "    # UI\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Pipeline-Start (Intelligente Checkpoint-Erkennung)</h3>\"),\n",
    "        info_box,\n",
    "        start_btn,\n",
    "        widgets.HTML(\"<hr><h4>Pipeline-Output:</h4>\"),\n",
    "        output_widget\n",
    "    ])\n",
    "    \n",
    "    display(ui)\n",
    "    \n",
    "    print(\"\\nPipeline-Start bereit!\")\n",
    "    if suggested_start:\n",
    "        print(f\"   Klicken Sie 'Pipeline starten' um bei '{suggested_start}' zu beginnen.\\n\")\n",
    "    else:\n",
    "        print(\"   Pipeline ist bereits komplett!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb3e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP-Zelle - TAG 1 ENDET HIER!\n",
    "\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Zeige STOP-Warnung\n",
    "stop_message = HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 30px; border: 4px solid #ff9800; text-align: center;\">\n",
    "<h1 style=\"color: #f57c00; margin: 0;\">STOP - TAG 1 ENDET HIER!</h1>\n",
    "<hr>\n",
    "<h3>Sie haben die GUIs fuer TAG 1 geladen:</h3>\n",
    "<p style=\"font-size: 18px;\">\n",
    "[+] Zelle 8: Marker-Konfiguration<br>\n",
    "[+] Zelle 9: Pipeline-Start\n",
    "</p>\n",
    "<hr>\n",
    "<h3>NAECHSTE SCHRITTE:</h3>\n",
    "<ol style=\"font-size: 16px; text-align: left; max-width: 600px; margin: auto;\">\n",
    "<li><b>Zelle 8:</b> Marker auswaehlen \u00e2\u2020\u2019 \"Speichern & Bereit\"</li>\n",
    "<li><b>Zelle 9:</b> \"Pipeline starten\" klicken</li>\n",
    "<li>Warten Sie bis Pipeline komplett durchgelaufen ist</li>\n",
    "</ol>\n",
    "<hr>\n",
    "<h3 style=\"color: #d32f2f;\">WARNUNG:</h3>\n",
    "<p style=\"font-size: 16px;\">\n",
    "Die Zellen UNTERHALB sind fuer TAG 2+ (Folge-Sessions)!<br>\n",
    "<b>Fuehren Sie diese NICHT aus waehrend TAG 1 laeuft!</b>\n",
    "</p>\n",
    "<hr>\n",
    "<p style=\"font-size: 14px; color: #666;\">\n",
    "Falls Sie \"Run All\" benutzt haben: Das ist okay!<br>\n",
    "Diese Zelle stoppt die automatische Ausfuehrung hier.\n",
    "</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "display(stop_message)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TAG 1 WORKFLOW BEREIT!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGehen Sie zu:\")\n",
    "print(\"  1. Zelle 8: Marker konfigurieren\")\n",
    "print(\"  2. Zelle 9: Pipeline starten\")\n",
    "print(\"\\nDie Zellen unterhalb (12-13) sind fuer TAG 2+ (spaetere Sessions).\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# WICHTIG: Verhindere weiteres \"Run All\"\n",
    "# Kommentieren Sie diese Zeile aus, wenn Sie Zelle 12 ausfuehren wollen:\n",
    "raise SystemExit(\"STOP: TAG 1 Setup komplett. Verwenden Sie Zelle 8 und 9.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558c408",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ?? Zelle 12: Erweiterte Pipeline-Steuerung (TAG 2+ oder Fehlerkorrektur)\n",
    "\n",
    "**?? NUR VERWENDEN WENN:**\n",
    "\n",
    "**Szenario A - Neue Marker hinzuf?gen (TAG 2+):**\n",
    "- Sie haben TAG 1 komplett abgeschlossen\n",
    "- Sie wollen zus?tzliche Marker hinzuf?gen\n",
    "- Alte Gating-Ergebnisse sollen erhalten bleiben\n",
    "\n",
    "**Szenario B - Modul neu starten (Fehlerkorrektur):**\n",
    "- Ein Modul ist fehlgeschlagen oder Ergebnis ist nicht korrekt\n",
    "- Sie wollen nur dieses Modul (und nachfolgende) neu laufen lassen\n",
    "- Vorherige Module sollen NICHT neu berechnet werden\n",
    "\n",
    "---\n",
    "\n",
    "**Diese GUI bietet:**\n",
    "- ? **Neue Marker extrahieren & mergen** (wie Zelle 12 vorher)\n",
    "- ? **Vollst?ndige Pipeline-Modul-Kontrolle** (wie Zelle 8)\n",
    "- ? **Checkpoint-Reset f?r einzelne Module** (z.B. nur `gating` neu starten)\n",
    "\n",
    "---\n",
    "\n",
    "**Workflow TAG 2+ (Neue Marker):**\n",
    "```\n",
    "1. Kernel ? Restart\n",
    "2. Nur Zellen 2-3 ausf?hren\n",
    "3. Diese Zelle ausf?hren ? GUI ?ffnet sich\n",
    "4. Tab 1: Neue Marker ausw?hlen ? \"Extract & Merge\"\n",
    "5. Tab 2: Nur \"aggregateData\" + \"gating\" ausw?hlen ? \"Run Selected\"\n",
    "```\n",
    "\n",
    "**Workflow Fehlerkorrektur (Modul neu starten):**\n",
    "```\n",
    "1. Kernel ? Restart\n",
    "2. Nur Zellen 2-3 ausf?hren\n",
    "3. Diese Zelle ausf?hren ? GUI ?ffnet sich\n",
    "4. Tab 2: \"Reset Checkpoint\" f?r fehlerhaftes Modul klicken\n",
    "5. Tab 2: Fehlerhaftes Modul + nachfolgende ausw?hlen ? \"Run Selected\"\n",
    "```\n",
    "\n",
    "**?? Bei ERSTER SESSION (TAG 1): Verwenden Sie Zelle 8 + Zelle 10!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f306b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 12: Erweiterte Pipeline-Steuerung (TAG 2+ / Fehlerkorrektur) - MIT INTELLIGENTER CHECKPOINT-ERKENNUNG\n",
    "\n",
    "# IMPORTS FIRST!\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from skimage.measure import regionprops_table\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ERWEITERTE PIPELINE-STEUERUNG (TAG 2+ / FEHLERKORREKTUR)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Mit intelligenter Checkpoint-Erkennung!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ueberpruefe benoetigte Variablen\n",
    "if 'BASE_DIR' not in locals():\n",
    "    BASE_DIR = Path.cwd()\n",
    "if 'CONFIG_PATH' not in locals():\n",
    "    CONFIG_PATH = BASE_DIR / \"cylinter_config.yml\"\n",
    "if 'CYLINTER_ENV_PATH' not in locals():\n",
    "    CYLINTER_ENV_PATH = Path(\"/opt/anaconda3/envs/cylinter_ben\")\n",
    "if 'CHECKPOINT_DIR' not in locals():\n",
    "    CHECKPOINT_DIR = BASE_DIR / \"cylinter_output_prune_test\" / \"checkpoints\"\n",
    "if 'MARKERS_CSV_PATH' not in locals():\n",
    "    MARKERS_CSV_PATH = BASE_DIR / \"markers.csv\"\n",
    "\n",
    "# CyLinter Executables\n",
    "cylinter_executable = CYLINTER_ENV_PATH / \"bin\" / \"cylinter\"\n",
    "python_executable = CYLINTER_ENV_PATH / \"bin\" / \"python\"\n",
    "\n",
    "# Alle Module (Pipeline-Reihenfolge)\n",
    "ALL_MODULES = [\n",
    "    \"aggregateData\", \"selectROIs\", \"intensityFilter\", \"areaFilter\",\n",
    "    \"cycleCorrelation\", \"logTransform\", \"pruneOutliers\", \"metaQC\", \"PCA\",\n",
    "    \"setContrast\", \"gating\", \"clustering\", \"clustermap\", \"frequencyStats\", \"curateThumbnails\"\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# TAB 1: NEUE MARKER EXTRAHIEREN & MERGEN (TAG 2)\n",
    "# ============================================================================\n",
    "\n",
    "# Lade markers.csv\n",
    "df_markers = pd.read_csv(MARKERS_CSV_PATH)\n",
    "bio_markers = df_markers[\n",
    "    ~df_markers['marker_name'].str.contains('DAPI', case=False, na=False) &\n",
    "    ~df_markers['marker_name'].str.contains('AF[12]_', case=True, na=False, regex=True)\n",
    "]['marker_name'].tolist()\n",
    "\n",
    "# Output fuer Marker-Extraktion (KEINE Hoehenbegrenzung!)\n",
    "marker_output = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ddd', padding='10px', margin='10px 0'\n",
    "))\n",
    "\n",
    "# Checkboxen fuer Marker\n",
    "bio_checkboxes = {}\n",
    "bio_checkbox_widgets = []\n",
    "for marker in sorted(bio_markers):\n",
    "    cb = widgets.Checkbox(value=False, description=marker, indent=False,\n",
    "                          layout=widgets.Layout(width='220px'))\n",
    "    bio_checkboxes[marker] = cb\n",
    "    bio_checkbox_widgets.append(cb)\n",
    "\n",
    "bio_grid = widgets.GridBox(bio_checkbox_widgets, layout=widgets.Layout(\n",
    "    grid_template_columns='repeat(3, 220px)', grid_gap='5px',\n",
    "    margin='10px 0'\n",
    "))\n",
    "\n",
    "# Buttons fuer Marker-Tab\n",
    "marker_select_all = widgets.Button(description='[*] Select All', layout=widgets.Layout(width='150px'))\n",
    "marker_deselect_all = widgets.Button(description='[ ] Deselect All', layout=widgets.Layout(width='150px'))\n",
    "\n",
    "def marker_select_all_click(_):\n",
    "    for cb in bio_checkboxes.values():\n",
    "        cb.value = True\n",
    "\n",
    "def marker_deselect_all_click(_):\n",
    "    for cb in bio_checkboxes.values():\n",
    "        cb.value = False\n",
    "\n",
    "marker_select_all.on_click(marker_select_all_click)\n",
    "marker_deselect_all.on_click(marker_deselect_all_click)\n",
    "\n",
    "marker_controls = widgets.HBox([marker_select_all, marker_deselect_all])\n",
    "\n",
    "# Extract & Merge Button\n",
    "extract_btn = widgets.Button(\n",
    "    description='[>] Extract & Merge New Markers',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='280px', height='45px'),\n",
    "    icon='flask'\n",
    ")\n",
    "\n",
    "def extract_and_merge_markers(b):\n",
    "    \"\"\"Extrahiert neue Marker und merged sie mit bestehender CSV\"\"\"\n",
    "    with marker_output:\n",
    "        marker_output.clear_output()\n",
    "        \n",
    "        selected_markers = [marker for marker, cb in bio_checkboxes.items() if cb.value]\n",
    "        \n",
    "        if not selected_markers:\n",
    "            print(\"WARNUNG: KEINE MARKER AUSGEWAEHLT!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MARKER-EXTRAKTION & MERGE\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        print(f\"Ausgewaehlte Marker: {len(selected_markers)}\")\n",
    "        \n",
    "        try:\n",
    "            # Lade Config\n",
    "            with open(CONFIG_PATH, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            \n",
    "            inDir = BASE_DIR / config.get('inDir', '.')\n",
    "            csv_dir = config.get('csv_dir', 'csv')\n",
    "            smd = config['sampleMetadata']\n",
    "            sample_name = list(smd.values())[0][0]\n",
    "            \n",
    "            csv_path = inDir / csv_dir / f\"{sample_name}.csv\"\n",
    "            tif_path = inDir / 'tif' / f\"{sample_name}.ome.tif\"\n",
    "            mask_path = inDir / 'mask' / f\"{sample_name}.tif\"\n",
    "            \n",
    "            print(f\"[+] Sample: {sample_name}\")\n",
    "            \n",
    "            # Lade alte CSV\n",
    "            if not csv_path.exists():\n",
    "                print(f\"[!] CSV nicht gefunden: {csv_path}\")\n",
    "                return\n",
    "            \n",
    "            old_csv = pd.read_csv(csv_path)\n",
    "            print(f\"[+] Alte CSV geladen: {len(old_csv.columns)} Spalten\")\n",
    "            \n",
    "            # Extrahiere neue Marker aus TIF\n",
    "            print(f\"\\n[*] Extrahiere {len(selected_markers)} neue Marker...\")\n",
    "            \n",
    "            tif_data = tifffile.imread(tif_path)\n",
    "            mask_data = tifffile.imread(mask_path)\n",
    "            \n",
    "            marker_indices = [i for i, m in enumerate(df_markers['marker_name']) if m in selected_markers]\n",
    "            \n",
    "            print(f\"[+] TIF Shape: {tif_data.shape}\")\n",
    "            print(f\"[+] Mask Shape: {mask_data.shape}\")\n",
    "            \n",
    "            new_data = {}\n",
    "            for idx, marker in zip(marker_indices, selected_markers):\n",
    "                channel_data = tif_data[idx]\n",
    "                props = regionprops_table(mask_data, intensity_image=channel_data,\n",
    "                                         properties=['label', 'mean_intensity'])\n",
    "                new_data[marker] = dict(zip(props['label'], props['mean_intensity']))\n",
    "                print(f\"  [+] {marker}: {len(props['label'])} Zellen\")\n",
    "            \n",
    "            # Merge mit alter CSV\n",
    "            print(f\"\\n[*] Merge mit bestehender CSV...\")\n",
    "            \n",
    "            new_columns_df = pd.DataFrame(new_data)\n",
    "            new_columns_df.index = old_csv.index\n",
    "            \n",
    "            merged_csv = pd.concat([old_csv, new_columns_df], axis=1)\n",
    "            \n",
    "            print(f\"[+] Alte CSV: {len(old_csv.columns)} Spalten\")\n",
    "            print(f\"[+] Neue Spalten: {len(selected_markers)}\")\n",
    "            print(f\"[+] Merged CSV: {len(merged_csv.columns)} Spalten\")\n",
    "            \n",
    "            # Backup & Save\n",
    "            from datetime import datetime\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            backup_path = csv_path.with_suffix(f'.csv.backup_{timestamp}')\n",
    "            \n",
    "            import shutil\n",
    "            shutil.copy2(csv_path, backup_path)\n",
    "            print(f\"[+] Backup: {backup_path.name}\")\n",
    "            \n",
    "            merged_csv.to_csv(csv_path, index=False)\n",
    "            print(f\"[+] CSV gespeichert: {csv_path.name}\")\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"ERFOLGREICH!\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            print(f\"[>] NAECHSTER SCHRITT:\")\n",
    "            print(f\"   1. Tab 2: Loesche aggregateData.parquet\")\n",
    "            print(f\"   2. Tab 2: Starte Pipeline bei aggregateData\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[!] FEHLER: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "extract_btn.on_click(extract_and_merge_markers)\n",
    "\n",
    "# Tab 1 Layout mit Scrollbar\n",
    "tab1_title = widgets.HTML(\"<h3>Neue Marker hinzufuegen (TAG 2+)</h3>\")\n",
    "tab1_info = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107;\">\n",
    "<b>Zweck:</b> Zusaetzliche Marker zur bestehenden CSV hinzufuegen<br><br>\n",
    "<b>Workflow:</b><br>\n",
    "1. Waehlen Sie neue Marker aus<br>\n",
    "2. Klicken Sie \"Extract & Merge\"<br>\n",
    "3. Gehen Sie zu Tab 2 \u00e2\u2020\u2019 Loeschen Sie aggregateData Checkpoint<br>\n",
    "4. Starten Sie Pipeline bei aggregateData\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "tab1_content = widgets.VBox([\n",
    "    tab1_title,\n",
    "    tab1_info,\n",
    "    marker_controls,\n",
    "    bio_grid,\n",
    "    extract_btn,\n",
    "    marker_output\n",
    "])  # KEINE Hoehenbegrenzung mehr!\n",
    "\n",
    "# ============================================================================\n",
    "# TAB 2: CHECKPOINT-RESET & INTELLIGENTE PIPELINE-START\n",
    "# ============================================================================\n",
    "\n",
    "# Output fuer Pipeline (KEINE Hoehenbegrenzung!)\n",
    "pipeline_output = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ccc', padding='10px', margin='10px 0'\n",
    "))\n",
    "\n",
    "# Checkpoint Reset Buttons (fuer jedes Modul)\n",
    "reset_buttons = {}\n",
    "reset_button_widgets = []\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Report Path fuer Gating-Reset\n",
    "REPORT_PATH = BASE_DIR / \"cylinter_output_prune_test\" / \"cylinter_report.yml\"\n",
    "\n",
    "for module in ALL_MODULES:\n",
    "    btn = widgets.Button(\n",
    "        description=f'[X] {module}',\n",
    "        button_style='danger',\n",
    "        layout=widgets.Layout(width='220px'),\n",
    "        tooltip=f'Loescht Checkpoint fuer {module}' if module != 'gating' else 'Loescht Checkpoint UND Report-Daten fuer Gating'\n",
    "    )\n",
    "    \n",
    "    def make_reset_handler(mod):\n",
    "        def handler(_):\n",
    "            with pipeline_output:\n",
    "                checkpoint_file = CHECKPOINT_DIR / f\"{mod}.parquet\"\n",
    "                \n",
    "                # Loesche Checkpoint\n",
    "                if checkpoint_file.exists():\n",
    "                    checkpoint_file.unlink()\n",
    "                    print(f\"[+] Checkpoint geloescht: {mod}.parquet\")\n",
    "                    print(f\"  -> Modul '{mod}' wird beim naechsten Run neu berechnet\")\n",
    "                else:\n",
    "                    print(f\"[i] Kein Checkpoint vorhanden: {mod}.parquet\")\n",
    "                \n",
    "                # SPEZIAL: Fuer Gating auch Report-Daten loeschen!\n",
    "                if mod == 'gating' and REPORT_PATH.exists():\n",
    "                    try:\n",
    "                        import yaml\n",
    "                        with open(REPORT_PATH, 'r') as f:\n",
    "                            report = yaml.safe_load(f)\n",
    "                        \n",
    "                        if 'gating' in report and report['gating']:\n",
    "                            num_gates = len(report['gating'])\n",
    "                            report['gating'] = {}\n",
    "                            \n",
    "                            with open(REPORT_PATH, 'w') as f:\n",
    "                                yaml.dump(report, f, default_flow_style=False)\n",
    "                            \n",
    "                            print(f\"[+] {num_gates} Gating-Eintraege aus Report geloescht\")\n",
    "                            print(f\"  -> Gating-GUI wird sich beim naechsten Run oeffnen!\")\n",
    "                        else:\n",
    "                            print(f\"[i] Keine Gating-Daten im Report vorhanden\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[!] Fehler beim Loeschen der Report-Daten: {e}\")\n",
    "                \n",
    "                print()\n",
    "        return handler\n",
    "    \n",
    "    btn.on_click(make_reset_handler(module))\n",
    "    reset_buttons[module] = btn\n",
    "    reset_button_widgets.append(btn)\n",
    "\n",
    "reset_grid = widgets.GridBox(reset_button_widgets, layout=widgets.Layout(\n",
    "    grid_template_columns='repeat(3, 220px)',\n",
    "    grid_gap='10px',\n",
    "    margin='10px 0'\n",
    "))\n",
    "\n",
    "# INTELLIGENTE CHECKPOINT-ERKENNUNG\n",
    "existing_checkpoints = []\n",
    "missing_checkpoints = []\n",
    "\n",
    "for module in ALL_MODULES:\n",
    "    checkpoint_file = CHECKPOINT_DIR / f\"{module}.parquet\"\n",
    "    if checkpoint_file.exists():\n",
    "        existing_checkpoints.append(module)\n",
    "    else:\n",
    "        missing_checkpoints.append(module)\n",
    "\n",
    "# Bestimme Vorschlag\n",
    "if len(missing_checkpoints) == 0:\n",
    "    suggested_start = ALL_MODULES[0]  # aggregateData als Default\n",
    "else:\n",
    "    suggested_start = missing_checkpoints[0]\n",
    "\n",
    "# Dropdown fuer Modul-Start (mit Vorschlag)\n",
    "module_dropdown = widgets.Dropdown(\n",
    "    options=ALL_MODULES,\n",
    "    value=suggested_start,\n",
    "    description='Start bei:',\n",
    "    layout=widgets.Layout(width='300px'),\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "# Pipeline Start Button\n",
    "run_from_module_btn = widgets.Button(\n",
    "    description='[>] Pipeline starten',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px', height='40px'),\n",
    "    icon='play',\n",
    "    tooltip='Startet Pipeline ab gewaehltem Modul'\n",
    ")\n",
    "\n",
    "def run_from_module(b):\n",
    "    \"\"\"Startet Pipeline ab gewaehltem Modul - MIT INTELLIGENTER CHECKPOINT-VALIDIERUNG\"\"\"\n",
    "    with pipeline_output:\n",
    "        pipeline_output.clear_output()\n",
    "        \n",
    "        start_module = module_dropdown.value\n",
    "        \n",
    "        # Re-check Checkpoints\n",
    "        missing = []\n",
    "        for module in ALL_MODULES:\n",
    "            if not (CHECKPOINT_DIR / f\"{module}.parquet\").exists():\n",
    "                missing.append(module)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"STARTE PIPELINE AB '{start_module}'\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        print(\"Checkpoint-Status:\")\n",
    "        for i, mod in enumerate(ALL_MODULES, 1):\n",
    "            status = \"[OK]\" if mod not in missing else \"[--]\"\n",
    "            marker = \">>>\" if mod == start_module else \"   \"\n",
    "            print(f\"{marker} {i:2}. {status} {mod}\")\n",
    "        print()\n",
    "        \n",
    "        # INTELLIGENTE VALIDIERUNG: Finde tatsaechliches Startmodul\n",
    "        actual_start_module = start_module\n",
    "        \n",
    "        if start_module != \"aggregateData\":\n",
    "            module_index = ALL_MODULES.index(start_module)\n",
    "            previous_module = ALL_MODULES[module_index - 1]\n",
    "            previous_checkpoint = CHECKPOINT_DIR / f\"{previous_module}.parquet\"\n",
    "            \n",
    "            if not previous_checkpoint.exists():\n",
    "                print(f\"[!] WARNUNG: Vorheriges Checkpoint '{previous_module}.parquet' fehlt!\")\n",
    "                print(f\"[!] CyLinter kann nicht bei '{start_module}' starten ohne vorheriges Checkpoint.\")\n",
    "                print(f\"[!] Suche nach letztem existierenden Checkpoint...\\n\")\n",
    "                \n",
    "                # Finde letztes existierendes Checkpoint VOR dem gewaehlten Modul\n",
    "                found_checkpoint = None\n",
    "                for i in range(module_index - 1, -1, -1):\n",
    "                    check_module = ALL_MODULES[i]\n",
    "                    check_file = CHECKPOINT_DIR / f\"{check_module}.parquet\"\n",
    "                    if check_file.exists():\n",
    "                        found_checkpoint = ALL_MODULES[i + 1]  # Naechstes Modul nach letztem Checkpoint\n",
    "                        break\n",
    "                \n",
    "                if found_checkpoint:\n",
    "                    actual_start_module = found_checkpoint\n",
    "                    print(f\"[+] Letztes Checkpoint gefunden: {ALL_MODULES[i]}.parquet\")\n",
    "                    print(f\"[+] Starte stattdessen bei: {actual_start_module}\")\n",
    "                else:\n",
    "                    actual_start_module = \"aggregateData\"\n",
    "                    print(f\"[!] Keine vorherigen Checkpoints gefunden!\")\n",
    "                    print(f\"[+] Starte bei: aggregateData (komplette Pipeline)\")\n",
    "                \n",
    "                print(f\"\\n[i] Pipeline laeuft von '{actual_start_module}' bis zum Ende\")\n",
    "                print(f\"[i] '{start_module}' wird dabei erreicht und neu berechnet.\\n\")\n",
    "        \n",
    "        # BUGFIX: Verwende actual_start_module statt start_module!\n",
    "        if cylinter_executable.exists():\n",
    "            cmd = [str(cylinter_executable), \"--module\", actual_start_module, str(CONFIG_PATH)]\n",
    "            print(f\"[*] Command: cylinter --module {actual_start_module} config.yml\")\n",
    "        else:\n",
    "            cmd = [str(python_executable), \"-m\", \"cylinter.cylinter\", \"--module\", actual_start_module, str(CONFIG_PATH)]\n",
    "            print(f\"[*] Command: python -m cylinter.cylinter --module {actual_start_module} config.yml\")\n",
    "        \n",
    "        print(f\"\\n[!] Pipeline laeuft von '{actual_start_module}' bis zum Ende!\")\n",
    "        print(f\"[!] CyLinter ueberschreibt alle Checkpoints ab '{actual_start_module}'!\\n\")\n",
    "        \n",
    "        overall_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            env = os.environ.copy()\n",
    "            process = subprocess.Popen(\n",
    "                cmd, cwd=str(BASE_DIR), env=env,\n",
    "                stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True, bufsize=1\n",
    "            )\n",
    "            \n",
    "            # Stream Output\n",
    "            for line in process.stdout:\n",
    "                print(line, end='')\n",
    "            \n",
    "            return_code = process.wait()\n",
    "            total_duration = time.time() - overall_start\n",
    "            \n",
    "            # Zusammenfassung\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            if return_code == 0:\n",
    "                print(f\"PIPELINE ERFOLGREICH!\")\n",
    "                print(f\"   Laufzeit: {total_duration:.1f}s ({total_duration/60:.1f} Min)\")\n",
    "            else:\n",
    "                print(f\"PIPELINE FEHLER (Exit Code: {return_code})\")\n",
    "                print(f\"   Laufzeit: {total_duration:.1f}s ({total_duration/60:.1f} Min)\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            \n",
    "            # Zeige neue Checkpoint-Status\n",
    "            print(\"Checkpoint-Status nach Run:\")\n",
    "            for module in ALL_MODULES:\n",
    "                cp_file = CHECKPOINT_DIR / f\"{module}.parquet\"\n",
    "                status = \"[OK]\" if cp_file.exists() else \"[--]\"\n",
    "                print(f\"   {status} {module}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[!] FEHLER: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "run_from_module_btn.on_click(run_from_module)\n",
    "\n",
    "# Tab 2 Layout mit Checkpoint-Info und Scrollbar\n",
    "checkpoint_info_html = f\"\"\"\n",
    "<div style=\"background-color: #e7f3ff; padding: 10px; border-left: 4px solid #2196F3; margin-bottom: 10px;\">\n",
    "<b>Checkpoint-Status:</b><br>\n",
    "Existierend: {len(existing_checkpoints)}/15<br>\n",
    "Fehlend: {len(missing_checkpoints)}/15<br><br>\n",
    "<b>Vorgeschlagener Start:</b> <code>{suggested_start}</code><br><br>\n",
    "<b>Hinweis:</b> CyLinter ueberschreibt alle Checkpoints ab dem Startmodul!\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "tab2_title = widgets.HTML(\"<h3>Checkpoint-Reset & Pipeline-Start</h3>\")\n",
    "tab2_checkpoint_info = widgets.HTML(checkpoint_info_html)\n",
    "tab2_info = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107;\">\n",
    "<b>Funktionen:</b><br>\n",
    "\u00e2\u20ac\u00a2 <b>Checkpoint loeschen:</b> Klicken Sie auf Modul-Button oben<br>\n",
    "\u00e2\u20ac\u00a2 <b>Pipeline starten:</b> Waehlen Sie Startmodul im Dropdown \u00e2\u2020\u2019 \"Pipeline starten\"<br><br>\n",
    "<b>WICHTIG:</b><br>\n",
    "\u00e2\u20ac\u00a2 Pipeline laeuft vom Startmodul bis zum Ende<br>\n",
    "\u00e2\u20ac\u00a2 Vorheriges Checkpoint muss existieren (ausser bei aggregateData)<br>\n",
    "\u00e2\u20ac\u00a2 INTELLIGENT: Wenn Checkpoint fehlt, startet bei letztem vorhandenen Checkpoint<br>\n",
    "\u00e2\u20ac\u00a2 Alle Checkpoints ab Startmodul werden ueberschrieben\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "tab2_content = widgets.VBox([\n",
    "    tab2_title,\n",
    "    tab2_checkpoint_info,\n",
    "    widgets.HTML(\"<h4>Checkpoint-Reset:</h4>\"),\n",
    "    reset_grid,\n",
    "    widgets.HTML(\"<hr><h4>Pipeline starten:</h4>\"),\n",
    "    tab2_info,\n",
    "    widgets.HBox([module_dropdown, run_from_module_btn]),\n",
    "    widgets.HTML(\"<hr><h4>Pipeline-Output:</h4>\"),\n",
    "    pipeline_output\n",
    "])  # KEINE Hoehenbegrenzung mehr!\n",
    "\n",
    "# ============================================================================\n",
    "# TABS KOMBINIEREN MIT SCROLLBAR\n",
    "# ============================================================================\n",
    "\n",
    "tabs = widgets.Tab(children=[tab1_content, tab2_content])\n",
    "tabs.set_title(0, 'Neue Marker (TAG 2+)')\n",
    "tabs.set_title(1, 'Checkpoint & Pipeline')\n",
    "\n",
    "main_title = widgets.HTML(\"<h2>Erweiterte Pipeline-Steuerung</h2>\")\n",
    "main_info = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 15px; border-left: 4px solid #ffc107;\">\n",
    "<b>Diese GUI ist fuer fortgeschrittene Workflows:</b><br><br>\n",
    "<b>Use Cases:</b><br>\n",
    "\u00e2\u20ac\u00a2 <b>TAG 2+:</b> Neue Marker hinzufuegen (Tab 1 \u00e2\u2020\u2019 Tab 2)<br>\n",
    "\u00e2\u20ac\u00a2 <b>Fehlerkorrektur:</b> Module neu berechnen (Tab 2: Checkpoint loeschen \u00e2\u2020\u2019 Pipeline starten)<br>\n",
    "\u00e2\u20ac\u00a2 <b>Ab Modul starten:</b> Pipeline ab bestimmtem Punkt fortsetzen (Tab 2)<br><br>\n",
    "<b>INTELLIGENT:</b> Wenn vorheriges Checkpoint fehlt, startet automatisch bei letztem vorhandenen!<br><br>\n",
    "<b>Fuer TAG 1 (erste Analyse):</b> Verwenden Sie Zelle 8 + Zelle 9!\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Main Container - KEINE Hoehenbegrenzung!\n",
    "ui = widgets.VBox([\n",
    "    main_title, \n",
    "    main_info, \n",
    "    tabs\n",
    "])\n",
    "\n",
    "# Display\n",
    "display(ui)\n",
    "\n",
    "print(\"\\nERWEITERTE PIPELINE-STEUERUNGS-GUI BEREIT!\")\n",
    "print(f\"   [*] Tab 1: {len(bio_markers)} Bio-Marker verfuegbar\")\n",
    "print(f\"   [*] Tab 2: {len(ALL_MODULES)} Pipeline-Module\")\n",
    "print(f\"   [*] Tab 2: Checkpoints: {len(existing_checkpoints)} OK, {len(missing_checkpoints)} fehlend\")\n",
    "print(f\"   [*] Tab 2: Vorgeschlagener Start: {suggested_start}\")\n",
    "print(f\"   [*] Tab 2: INTELLIGENTE Validierung aktiviert!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac34a2",
   "metadata": {},
   "source": [
    "## ?? Parquet Viewer (optional)\n",
    "\n",
    "Optionales Werkzeug, um Checkpoint-Parquet-Dateien einzusehen. L?dt keine Pipeline; vorher Zelle 11 ausf?hren, damit `CHECKPOINT_DIR` gesetzt ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3be7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARQUET DATA VIEWER - Kopieren Sie diesen gesamten Code in eine neue Notebook-Zelle\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "checkpoint_dir = CHECKPOINT_DIR\n",
    "parquet_files = sorted(checkpoint_dir.glob('*.parquet'))\n",
    "parquet_names = [f.name for f in parquet_files]\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"\u00e2\u009d\u0152 No parquet files found!\")\n",
    "else:\n",
    "    print(f\"\u00e2\u0153\u2026 Found {len(parquet_files)} parquet file(s)\")\n",
    "\n",
    "current_data = None\n",
    "\n",
    "title_html = widgets.HTML(value=\"\"\"<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px;'><h2 style='color: white; margin: 0;'>\u00f0\u0178\u201c\u0160 Parquet Viewer</h2></div>\"\"\")\n",
    "\n",
    "file_dropdown = widgets.Dropdown(options=parquet_names, value=parquet_names[-1] if parquet_names else None, description='File:', layout=widgets.Layout(width='500px'))\n",
    "load_btn = widgets.Button(description='\u00f0\u0178\u201c\u201a Load', button_style='primary')\n",
    "info_output = widgets.Output()\n",
    "preview_output = widgets.Output(layout=widgets.Layout(max_height='400px', overflow_y='auto'))\n",
    "export_filename = widgets.Text(value='data.csv', description='Name:')\n",
    "export_btn = widgets.Button(description='\u00f0\u0178\u2019\u00be Export CSV', button_style='success')\n",
    "\n",
    "def load_data(b):\n",
    "    global current_data\n",
    "    with info_output:\n",
    "        info_output.clear_output()\n",
    "        try:\n",
    "            current_data = pd.read_parquet(checkpoint_dir / file_dropdown.value)\n",
    "            print(f\"\u00e2\u0153\u2026 Loaded {len(current_data):,} rows, {len(current_data.columns)} cols\")\n",
    "            with preview_output:\n",
    "                preview_output.clear_output()\n",
    "                display(current_data.head(10))\n",
    "        except Exception as e:\n",
    "            print(f\"\u00e2\u009d\u0152 Error: {e}\")\n",
    "\n",
    "def export_data(b):\n",
    "    with info_output:\n",
    "        info_output.clear_output()\n",
    "        if current_data is not None:\n",
    "            path = Path.cwd() / export_filename.value\n",
    "            current_data.to_csv(path, index=False)\n",
    "            print(f\"\u00e2\u0153\u2026 Exported to {path}\")\n",
    "        else:\n",
    "            print(\"\u00e2\u009d\u0152 Load data first!\")\n",
    "\n",
    "load_btn.on_click(load_data)\n",
    "export_btn.on_click(export_data)\n",
    "\n",
    "ui = widgets.VBox([title_html, widgets.HBox([file_dropdown, load_btn]), info_output, preview_output, widgets.HBox([export_filename, export_btn])], layout=widgets.Layout(padding='20px'))\n",
    "display(ui)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cylinter_ben",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}