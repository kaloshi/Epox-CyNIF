{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c869addf",
   "metadata": {},
   "source": [
    "# Spillover Removal \u00e2\u20ac\u201c Marker CSV (normalisierte Pipeline wie V5)\n",
    "\n",
    "Diese Version repliziert das Verhalten des klassischen Mosaic/Spillover-UI: \n",
    "- Komplett-Stack wird pro Kanal auf 0\u00e2\u20ac\u201c1 normalisiert.\n",
    "- Spillover-Koeffizienten werden via Mutual Information gesch\u00c3\u00a4tzt und auf den normalisierten Stack angewandt.\n",
    "- Danach wird jeder Kanal wieder in den urspr\u00c3\u00bcnglichen Wertebereich zur\u00c3\u00bccktransformiert und im Original-Datentyp gespeichert.\n",
    "\n",
    "Damit sollte die Spillover-Entfernung den bekannten Effekt zeigen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23fdf7",
   "metadata": {},
   "source": [
    "## 1. Sample Configuration (Dynamisch wie Script 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === \u00f0\u0178\u017d\u00af SAMPLE CONFIGURATION (DYNAMISCH wie Script 1) ===\n",
    "from pathlib import Path\n",
    "\n",
    "# === 1. SAMPLE-AUSWAHL ===\n",
    "current_sample = \"sample_004\"  # \u00e2\u2020\u0090 HIER SAMPLE \u00c3\u201eNDERN (muss mit Script 1 \u00c3\u00bcbereinstimmen)\n",
    "\n",
    "print(f\"\u00f0\u0178\u017d\u00af SAMPLE: {current_sample}\")\n",
    "\n",
    "# === 2. DYNAMISCHE PFAD-STRUKTUR (wie Script 1) ===\n",
    "# Pfade werden automatisch aus current_sample abgeleitet\n",
    "WORKSPACE_ROOT = Path(r\"C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\")\n",
    "BASE_EXPORT_ROOT = WORKSPACE_ROOT / \"data\" / \"export\"\n",
    "BASE_EXPORT = BASE_EXPORT_ROOT / current_sample\n",
    "\n",
    "# Legacy-Kompatibilit\u00c3\u00a4t\n",
    "WORKSPACE_EXPORT_ROOT = BASE_EXPORT_ROOT  \n",
    "SAMPLE_ID = current_sample\n",
    "\n",
    "print(f\"\u00f0\u0178\u201c\u201a WORKSPACE_ROOT: {WORKSPACE_ROOT}\")\n",
    "print(f\"\u00f0\u0178\u201c\u201a BASE_EXPORT_ROOT: {BASE_EXPORT_ROOT}\")  \n",
    "print(f\"\u00f0\u0178\u201c\u201a BASE_EXPORT (Sample): {BASE_EXPORT}\")\n",
    "\n",
    "# === 3. VALIDIERUNG ===\n",
    "if not BASE_EXPORT.exists():\n",
    "    raise RuntimeError(f\"\u00e2\u009d\u0152 FEHLER: Sample-Verzeichnis existiert nicht: {BASE_EXPORT}\")\n",
    "else:\n",
    "    print(f\"\u00e2\u0153\u2026 Sample-Verzeichnis gefunden\")\n",
    "\n",
    "# === 4. GLOBALE VARIABLEN SETZEN ===\n",
    "globals()[\"WORKSPACE_ROOT\"] = WORKSPACE_ROOT\n",
    "globals()[\"WORKSPACE_EXPORT_ROOT\"] = WORKSPACE_EXPORT_ROOT\n",
    "globals()[\"BASE_EXPORT_ROOT\"] = BASE_EXPORT_ROOT\n",
    "globals()[\"BASE_EXPORT\"] = BASE_EXPORT\n",
    "globals()[\"current_sample\"] = current_sample\n",
    "globals()[\"SAMPLE_ID\"] = SAMPLE_ID\n",
    "\n",
    "# === INPUT-FILE FINDER (STRENG: Nur aus multicycle_mosaics/decon2D_fused/) ===\n",
    "MULTICYCLE_DIR = BASE_EXPORT / \"multicycle_mosaics\"\n",
    "DECON_DIR = MULTICYCLE_DIR / \"decon2D\"\n",
    "FUSED_DIR = DECON_DIR / \"decon2D_fused\"\n",
    "\n",
    "if not FUSED_DIR.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"\u00e2\u009d\u0152 FEHLER: FUSED_DIR nicht gefunden: {FUSED_DIR}\\n\"\n",
    "        f\"   Erwartete Struktur: {BASE_EXPORT}/multicycle_mosaics/decon2D/decon2D_fused/\"\n",
    "    )\n",
    "\n",
    "# Suche EXPLICIT in FUSED_DIR (NICHT rekursiv!)\n",
    "input_candidates = sorted(FUSED_DIR.glob(\"fused_decon*.tif\"))  # Auch .tif ohne .ome akzeptieren\n",
    "if not input_candidates:\n",
    "    raise FileNotFoundError(\n",
    "        f\"\u00e2\u009d\u0152 FEHLER: Kein fused_decon*.tif in {FUSED_DIR} gefunden.\\n\"\n",
    "        f\"   Script 2 (Spillover) erwartet Input aus Part 1 (Stitching/Decon/EDF).\\n\"\n",
    "        f\"   Bitte Part 1 ausf\u00c3\u00bchren BEVOR Part 2 gestartet wird.\"\n",
    "    )\n",
    "if len(input_candidates) > 1:\n",
    "    raise RuntimeError(\n",
    "        f\"\u00e2\u009d\u0152 FEHLER: Mehrere fused_decon*.ome.tif gefunden in {FUSED_DIR}:\\n\"\n",
    "        + '\\n'.join(f\"   - {p.name}\" for p in input_candidates)\n",
    "        + \"\\n   Bitte unerw\u00c3\u00bcnschte Dateien entfernen oder umbenennen.\"\n",
    "    )\n",
    "INPUT_PATH = input_candidates[0]\n",
    "print(f\"\u00e2\u0153\u2026 Input-TIF gefunden: {INPUT_PATH.relative_to(BASE_EXPORT)}\")\n",
    "\n",
    "sample_token = ''.join(ch for ch in SAMPLE_ID if ch.isdigit()) or SAMPLE_ID\n",
    "marker_candidates = [\n",
    "    csv_path for csv_path in BASE_EXPORT.glob('markers_*.csv')\n",
    "    if sample_token.lower() in csv_path.stem.lower()\n",
    "]\n",
    "if not marker_candidates:\n",
    "    raise FileNotFoundError(\n",
    "        f'Erwartete Marker-CSV markers_{sample_token}.csv nicht in {BASE_EXPORT} gefunden.'\n",
    "    )\n",
    "if len(marker_candidates) > 1:\n",
    "    raise RuntimeError(\n",
    "        'Mehrere Marker-CSV-Dateien gefunden. Bitte unerw\u00c3\u00bcnschte Dateien entfernen: '\n",
    "        + ', '.join(str(p.name) for p in marker_candidates)\n",
    "    )\n",
    "MARKER_CSV = marker_candidates[0]\n",
    "\n",
    "ROI_CFG = dict(x=1696, y=4528, size=384)  # hart definierte ROI wie in V7\n",
    "ROI_PAD = 32  # zus\u00c3\u00a4tzlicher Kontext f\u00c3\u00bcr Koeffizienten-Sch\u00c3\u00a4tzung\n",
    "PERCENTILE_Q = 99.5  # Begrenzung \u00c3\u00bcber hohe Percentile (dynamischer Cap)\n",
    "SAFETY_FACTOR = 1.2  # Sicherheitsfaktor f\u00c3\u00bcr den Percentile-Cap\n",
    "MAX_COEFF = 3.0  # hartes globales Maximum als Sicherung\n",
    "\n",
    "SKIP_CHANNELS = [1, 2, 3]  # 1-basiert\n",
    "RNG_SEED = 42\n",
    "\n",
    "print('Konfiguration geladen. Input:', INPUT_PATH)\n",
    "print('Marker CSV:', MARKER_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1877120",
   "metadata": {},
   "source": [
    "## 2. Imports, Logging, Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ab23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from skimage import filters, morphology\n",
    "import tifffile as tiff\n",
    "from tifffile import TiffFile\n",
    "from ipywidgets import widgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)-8s | %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    " )\n",
    "logger = logging.getLogger('SpilloverOnly')\n",
    "logger.info('Logging initialisiert.')\n",
    "\n",
    "np.random.default_rng(RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "OME_NS = {'ome': 'http://www.openmicroscopy.org/Schemas/OME/2016-06'}\n",
    "\n",
    "\n",
    "def extract_channel_names(ome_xml: str, expected: int) -> List[str]:\n",
    "    names: List[str] = []\n",
    "    if ome_xml:\n",
    "        try:\n",
    "            root = ET.fromstring(ome_xml)\n",
    "            for ch in root.findall('.//ome:Channel', OME_NS):\n",
    "                name = ch.get('Name')\n",
    "                if name:\n",
    "                    names.append(name)\n",
    "        except ET.ParseError as exc:\n",
    "            logger.warning('OME-XML konnte nicht geparst werden: %s', exc)\n",
    "    if len(names) < expected:\n",
    "        names.extend([f'C{i+1}' for i in range(len(names), expected)])\n",
    "    return names[:expected]\n",
    "\n",
    "\n",
    "def load_ome_to_chw(path: Path) -> Tuple[np.ndarray, np.dtype, List[str]]:\n",
    "    logger.info('Lade OME-TIFF: %s', path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    \n",
    "    with TiffFile(str(path)) as tf:\n",
    "        ome_xml = tf.ome_metadata\n",
    "        num_pages = len(tf.pages)\n",
    "        \n",
    "        # KRITISCH: Pr\u00c3\u00bcfe ob Multi-Page TIFF (Channels als separate Pages)\n",
    "        if num_pages > 1:\n",
    "            logger.info(f'Multi-Page TIFF erkannt: {num_pages} Pages (Channels)')\n",
    "            # Lade ALLE Pages als separaten Stack\n",
    "            first_page = tf.pages[0]\n",
    "            orig_dtype = first_page.dtype\n",
    "            shape = first_page.shape\n",
    "            \n",
    "            # Preallocate Stack: (Channels, Y, X)\n",
    "            arr = np.empty((num_pages, shape[0], shape[1]), dtype=orig_dtype)\n",
    "            \n",
    "            # Lade jede Page in Stack\n",
    "            logger.info(f'Lade {num_pages} Channels...')\n",
    "            for i, page in enumerate(tf.pages):\n",
    "                arr[i] = page.asarray()\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    logger.info(f'  {i + 1}/{num_pages} Channels geladen')\n",
    "            \n",
    "            logger.info(f'\u00e2\u0153\u2026 Alle {num_pages} Channels geladen: Shape={arr.shape}, Dtype={orig_dtype}')\n",
    "            axes = 'CYX'\n",
    "        else:\n",
    "            # Single-Page oder OME mit Achsen\n",
    "            series = tf.series[0]\n",
    "            axes = series.axes\n",
    "            arr = series.asarray()\n",
    "            orig_dtype = arr.dtype\n",
    "            \n",
    "            # Entferne T und Z (nur erster Frame/Plane)\n",
    "            for ax in ('T', 'Z'):\n",
    "                if ax in axes:\n",
    "                    idx = axes.index(ax)\n",
    "                    arr = np.take(arr, 0, axis=idx)\n",
    "                    axes = axes.replace(ax, '')\n",
    "            \n",
    "            if 'C' not in axes:\n",
    "                arr = arr[None, ...]\n",
    "                axes = 'C' + axes\n",
    "                logger.warning('Keine C-Achse gefunden \u00e2\u20ac\u201c nehme erste Dimension als Kanal an.')\n",
    "            \n",
    "            c_idx = axes.index('C')\n",
    "            if c_idx != 0:\n",
    "                arr = np.moveaxis(arr, c_idx, 0)\n",
    "            \n",
    "            # Reduziere auf exakt 3D (C, Y, X)\n",
    "            while arr.ndim > 3:\n",
    "                arr = arr[0]\n",
    "    \n",
    "    channel_names = extract_channel_names(ome_xml, arr.shape[0])\n",
    "    return arr.astype(np.float32), orig_dtype, channel_names\n",
    "\n",
    "\n",
    "def extract_roi_patch(img: np.ndarray, *, x: int, y: int, size: int) -> Tuple[np.ndarray, Tuple[int, int, int, int]]:\n",
    "    size = int(size)\n",
    "    half = size // 2\n",
    "    y0 = max(0, int(y) - half)\n",
    "    x0 = max(0, int(x) - half)\n",
    "    y1 = min(img.shape[-2], y0 + size)\n",
    "    x1 = min(img.shape[-1], x0 + size)\n",
    "    if (y1 - y0) < size:\n",
    "        y0 = max(0, y1 - size)\n",
    "    if (x1 - x0) < size:\n",
    "        x0 = max(0, x1 - size)\n",
    "    return img[y0:y1, x0:x1], (y0, y1, x0, x1)\n",
    "\n",
    "\n",
    "def ls_coeff(donor: np.ndarray, target: np.ndarray) -> float:\n",
    "    donor_f = donor.astype(np.float32, copy=False)\n",
    "    target_f = target.astype(np.float32, copy=False)\n",
    "    denom = float(np.sum(donor_f * donor_f) + 1e-6)\n",
    "    if denom <= 0:\n",
    "        return 0.0\n",
    "    num = float(np.sum(donor_f * target_f))\n",
    "    return max(0.0, num / denom)\n",
    "\n",
    "\n",
    "def percentile_cap(donor: np.ndarray, target: np.ndarray) -> float:\n",
    "    if PERCENTILE_Q is None:\n",
    "        return float('inf')\n",
    "    donor_q = float(np.percentile(donor, PERCENTILE_Q))\n",
    "    target_q = float(np.percentile(target, PERCENTILE_Q))\n",
    "    if not np.isfinite(donor_q) or donor_q <= 1e-6:\n",
    "        return float('inf')\n",
    "    if not np.isfinite(target_q) or target_q < 0:\n",
    "        target_q = max(0.0, target_q)\n",
    "    cap = (target_q / donor_q) * float(SAFETY_FACTOR)\n",
    "    if cap < 0:\n",
    "        return 0.0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def final_cap(raw_coeff: float, ratio_cap: float) -> float:\n",
    "    coeff = raw_coeff\n",
    "    if ratio_cap is not None:\n",
    "        coeff = min(coeff, ratio_cap)\n",
    "    if MAX_COEFF is not None:\n",
    "        coeff = min(coeff, float(MAX_COEFF))\n",
    "    return max(0.0, coeff)\n",
    "\n",
    "\n",
    "def estimate_coeff_roi(target: np.ndarray, donor: np.ndarray, *, roi_cfg: Dict[str, int], pad: int) -> Tuple[float, Dict[str, Any]]:\n",
    "    base_cfg = dict(roi_cfg)\n",
    "    roi_target, coords = extract_roi_patch(target, **base_cfg)\n",
    "    roi_donor, _ = extract_roi_patch(donor, **base_cfg)\n",
    "    if pad and pad > 0:\n",
    "        pad_cfg = dict(base_cfg)\n",
    "        pad_cfg['size'] = base_cfg['size'] + 2 * int(pad)\n",
    "        roi_target, coords = extract_roi_patch(target, **pad_cfg)\n",
    "        roi_donor, _ = extract_roi_patch(donor, **pad_cfg)\n",
    "\n",
    "    coeff_raw = ls_coeff(roi_donor, roi_target)\n",
    "    cap_ratio = percentile_cap(roi_donor, roi_target) if coeff_raw > 0 else float('inf')\n",
    "    coeff = final_cap(coeff_raw, cap_ratio)\n",
    "\n",
    "    info = dict(\n",
    "        coeff=float(coeff),\n",
    "        coeff_raw=float(coeff_raw),\n",
    "        coeff_cap_ratio=(float(cap_ratio) if np.isfinite(cap_ratio) else None),\n",
    "        pad=int(pad),\n",
    "        roi_coords=dict(top=int(coords[0]), bottom=int(coords[1]), left=int(coords[2]), right=int(coords[3])),\n",
    "        roi_sum_raw=float(np.sum(roi_target)),\n",
    "        roi_sum_donor=float(np.sum(roi_donor)),\n",
    "    )\n",
    "    return coeff, info\n",
    "\n",
    "\n",
    "def apply_spillover(stack: np.ndarray, donor_idx: int, target_idx: int, coeff: float) -> None:\n",
    "    stack[target_idx] = np.clip(stack[target_idx] - coeff * stack[donor_idx], 0, None)\n",
    "\n",
    "\n",
    "def convert_to_dtype(stack: np.ndarray, dtype: np.dtype) -> np.ndarray:\n",
    "    if np.issubdtype(dtype, np.integer):\n",
    "        info = np.iinfo(dtype)\n",
    "        out = np.clip(stack, info.min, info.max)\n",
    "    else:\n",
    "        out = stack\n",
    "    return out.astype(dtype, copy=False)\n",
    "\n",
    "\n",
    "def build_output_base(input_path: Path) -> Tuple[Path, Path, Path, Path]:\n",
    "    \"\"\"\n",
    "    Erzeugt Output-Pfade im dedizierten spillover/-Ordner.\n",
    "    \n",
    "    Struktur:\n",
    "    BASE_EXPORT/spillover/\n",
    "        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac fused_decon_spillover_corrected.ome.tif\n",
    "        \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac spillover_coefficients.json\n",
    "        \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac spillover_channels/\n",
    "    \"\"\"\n",
    "    # Spillover-Ordner direkt in BASE_EXPORT (Root-bounded!)\n",
    "    spillover_dir = BASE_EXPORT / \"spillover\"\n",
    "    spillover_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Fixer Dateiname (kein Timestamp!)\n",
    "    stem = input_path.stem.replace('.ome', '')  # Entfernt .ome.tif\n",
    "    ome_path = spillover_dir / f\"{stem}_spillover_corrected.ome.tif\"\n",
    "    channel_dir = spillover_dir / \"spillover_channels\"\n",
    "    spill_path = spillover_dir / \"spillover_coefficients.json\"\n",
    "    \n",
    "    return spillover_dir, ome_path, channel_dir, spill_path\n",
    "\n",
    "\n",
    "def parse_marker_table(csv_path: Path) -> Tuple[List[Dict[str, Any]], Dict[str, int], Dict[int, Dict[str, Any]], List[Tuple[int, int]]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        logger.warning('Marker-CSV %s ist leer oder konnte nicht gelesen werden.', csv_path)\n",
    "        return [], {}, {}, []\n",
    "\n",
    "    headers = list(rows[0].keys())\n",
    "\n",
    "    def _find_col(tokens: List[str], default: Optional[str] = None) -> Optional[str]:\n",
    "        opts = [col for col in headers]\n",
    "        if default and default in opts:\n",
    "            return default\n",
    "        for col in opts:\n",
    "            low = col.lower()\n",
    "            if any(tok in low for tok in tokens):\n",
    "                return col\n",
    "        return default if default in headers else None\n",
    "\n",
    "    include_col = _find_col(['include'], None)\n",
    "\n",
    "    include_rows: List[Dict[str, Any]] = []\n",
    "    for row in rows:\n",
    "        flag = row.get(include_col, 'TRUE') if include_col else 'TRUE'\n",
    "        if str(flag).strip().upper() in {'TRUE', '1', 'YES', 'Y', 'JA'}:\n",
    "            include_rows.append(row)\n",
    "\n",
    "    if not include_rows:\n",
    "        logger.warning('Keine Include==TRUE-Eintr\u00c3\u00a4ge in Marker-CSV: %s', csv_path)\n",
    "        return [], {}, {}, []\n",
    "\n",
    "    no_col = _find_col(['no'], 'No')\n",
    "    marker_col = _find_col(['marker', 'antibody', 'target'], 'Marker-Name')\n",
    "    fluor_col = _find_col(['fluor', 'dye'], None)\n",
    "    spill_col = _find_col(['spillover'], 'Spillover_from')\n",
    "\n",
    "    mapping: Dict[str, int] = {}\n",
    "    for idx, row in enumerate(include_rows):\n",
    "        channel_no = str(row.get(no_col, '')).strip() if no_col else str(idx + 1)\n",
    "        if channel_no:\n",
    "            mapping[channel_no] = idx\n",
    "\n",
    "    channel_info: Dict[int, Dict[str, Any]] = {}\n",
    "    spill_pairs: List[Tuple[int, int]] = []\n",
    "\n",
    "    for idx, row in enumerate(include_rows):\n",
    "        channel_no = str(row.get(no_col, '')).strip() if no_col else str(idx + 1)\n",
    "        marker_name = str(row.get(marker_col, '')).strip() if marker_col else ''\n",
    "        fluor_name = str(row.get(fluor_col, '')).strip() if fluor_col else ''\n",
    "        spill_from = str(row.get(spill_col, '')).strip() if spill_col else ''\n",
    "\n",
    "        channel_info[idx] = dict(\n",
    "            no=channel_no or str(idx + 1),\n",
    "            marker=marker_name,\n",
    "            fluor=fluor_name,\n",
    "            spillover_from=spill_from,\n",
    "        )\n",
    "\n",
    "        donor_no = spill_from\n",
    "        if donor_no and donor_no.lower() != 'na':\n",
    "            donor_idx = mapping.get(donor_no)\n",
    "            if donor_idx is None:\n",
    "                logger.warning('Spillover-Referenz %s (f\u00c3\u00bcr Ziel No=%s) ist nicht in Include==TRUE enthalten.', donor_no, channel_no)\n",
    "                continue\n",
    "            spill_pairs.append((idx, donor_idx))\n",
    "\n",
    "    return include_rows, mapping, channel_info, spill_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6317ff6",
   "metadata": {},
   "source": [
    "## 3. CSV-Analyse & Index-Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92602166",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_rows, id_mapping, channel_info, spill_pairs = parse_marker_table(MARKER_CSV)\n",
    "logger.info('Include==TRUE: %d Kan\u00c3\u00a4le', len(include_rows))\n",
    "logger.info('Spillover-Paare (gemappt): %s', spill_pairs)\n",
    "\n",
    "skip_zero_based = [max(0, sc - 1) for sc in SKIP_CHANNELS]\n",
    "logger.info('\u00c3\u0153berspringe Kan\u00c3\u00a4le (0-based): %s', skip_zero_based)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95208fc",
   "metadata": {},
   "source": [
    "## 3.5 DIAGNOSE: Input-File pr\u00c3\u00bcfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Pr\u00c3\u00bcfe Input-File Gr\u00c3\u00b6\u00c3\u0178e\n",
    "input_size_bytes = INPUT_PATH.stat().st_size\n",
    "input_size_gb = input_size_bytes / (1024**3)\n",
    "print(f\"Input-File: {INPUT_PATH.name}\")\n",
    "print(f\"Input-Gr\u00c3\u00b6\u00c3\u0178e: {input_size_gb:.2f} GB ({input_size_bytes:,} Bytes)\")\n",
    "\n",
    "# DETAILLIERTE TIFF-ANALYSE\n",
    "with TiffFile(str(INPUT_PATH)) as tf:\n",
    "    print(f\"\\n\u00f0\u0178\u201d\u008d DETAILLIERTE TIFF-ANALYSE:\")\n",
    "    print(f\"  Anzahl Series: {len(tf.series)}\")\n",
    "    print(f\"  Anzahl Pages: {len(tf.pages)}\")\n",
    "    \n",
    "    # Erste Series (Standard)\n",
    "    series = tf.series[0]\n",
    "    print(f\"\\n  Series[0]:\")\n",
    "    print(f\"    Axes: {series.axes}\")\n",
    "    print(f\"    Shape: {series.shape}\")\n",
    "    print(f\"    Dtype: {series.dtype}\")\n",
    "    \n",
    "    # Pr\u00c3\u00bcfe, ob Pages separate Channels sind\n",
    "    if len(tf.pages) > 1:\n",
    "        print(f\"\\n  \u00e2\u0161\u00a0\u00ef\u00b8\u008f WARNUNG: {len(tf.pages)} Pages gefunden!\")\n",
    "        print(f\"     M\u00c3\u00b6glicherweise sind die Channels als separate Pages gespeichert.\")\n",
    "        print(f\"     Erste 3 Pages:\")\n",
    "        for i, page in enumerate(tf.pages[:3]):\n",
    "            print(f\"       Page {i}: Shape={page.shape}, Dtype={page.dtype}\")\n",
    "    \n",
    "    # Berechne erwartete Gr\u00c3\u00b6\u00c3\u0178e WENN 83 Channels\n",
    "    expected_bytes_single = 1\n",
    "    for dim in series.shape:\n",
    "        expected_bytes_single *= dim\n",
    "    expected_bytes_single *= np.dtype(series.dtype).itemsize\n",
    "    \n",
    "    expected_bytes_83ch = expected_bytes_single * 83\n",
    "    expected_gb_83ch = expected_bytes_83ch / (1024**3)\n",
    "    \n",
    "    print(f\"\\n  Erwartete Gr\u00c3\u00b6\u00c3\u0178e (1 Channel, unkomprimiert): {expected_bytes_single / (1024**3):.2f} GB\")\n",
    "    print(f\"  Erwartete Gr\u00c3\u00b6\u00c3\u0178e (83 Channels, unkomprimiert): {expected_gb_83ch:.2f} GB\")\n",
    "    \n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\u00f0\u0178\u0161\u00a8 DIAGNOSE:\")\n",
    "if len(tf.pages) > 80:\n",
    "    print(f\"  \u00e2\u0153\u2026 File hat {len(tf.pages)} Pages \u00e2\u2020\u2019 wahrscheinlich 83 Channels als separate Pages!\")\n",
    "    print(f\"  \u00e2\u009d\u0152 ABER: tifffile.series[0] liest nur ERSTE Page/Channel!\")\n",
    "    print(f\"  \u00f0\u0178\u201d\u00a7 FIX: load_ome_to_chw() muss ALLE Pages laden, nicht nur series[0]!\")\n",
    "else:\n",
    "    print(f\"  \u00e2\u009d\u0152 File hat NUR {len(tf.pages)} Pages \u00e2\u2020\u2019 Script 1 hat falsch gespeichert!\")\n",
    "    print(f\"  \u00f0\u0178\u201d\u00a7 FIX: Script 1 muss Channels als Multi-Page TIFF oder mit C-Achse speichern!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b33a6",
   "metadata": {},
   "source": [
    "## 4. Spillover entfernen & speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stack_raw, orig_dtype, channel_names = load_ome_to_chw(INPUT_PATH)\n",
    "if stack_raw.shape[0] != len(include_rows):\n",
    "    logger.warning('Stack-Kanalanzahl (%d) entspricht nicht Include==TRUE (%d). Bitte pr\u00c3\u00bcfen!', stack_raw.shape[0], len(include_rows))\n",
    "\n",
    "stack_work = stack_raw.copy()\n",
    "\n",
    "_unknown_tokens = {'nan', 'none', 'unknown', ''}\n",
    "\n",
    "def _clean_text(value: Any) -> str:\n",
    "    if value is None:\n",
    "        return ''\n",
    "    text = str(value).strip()\n",
    "    if not text or text.lower() in _unknown_tokens:\n",
    "        return ''\n",
    "    return text\n",
    "\n",
    "def _channel_label(idx: int) -> str:\n",
    "    info = channel_info.get(idx, {})\n",
    "    marker = _clean_text(info.get('marker'))\n",
    "    fluor = _clean_text(info.get('fluor'))\n",
    "    parts = [p for p in (marker, fluor) if p]\n",
    "    if parts:\n",
    "        return ' | '.join(parts)\n",
    "    if idx < len(channel_names):\n",
    "        return channel_names[idx]\n",
    "    return f'C{idx+1:02d}'\n",
    "\n",
    "def _safe_token(text: str) -> str:\n",
    "    cleaned = text.replace('|', '_').replace('/', '_').replace('\\\\', '_')\n",
    "    cleaned = cleaned.replace(' ', '_')\n",
    "    return ''.join(ch if ch.isalnum() or ch in {'_', '-'} else '_' for ch in cleaned)\n",
    "\n",
    "channel_labels = [_channel_label(i) for i in range(stack_work.shape[0])]\n",
    "\n",
    "logger.info('Starte Spillover-Korrektur mit ROI-LS (V7-Mechanik).')\n",
    "spill_results: List[Dict[str, Any]] = []\n",
    "for target_idx, donor_idx in tqdm(spill_pairs, desc='Spillover'):\n",
    "    if target_idx >= stack_work.shape[0] or donor_idx >= stack_work.shape[0]:\n",
    "        logger.warning('Paar au\u00c3\u0178erhalb des Stacks: target=%d donor=%d', target_idx, donor_idx)\n",
    "        continue\n",
    "    if target_idx in skip_zero_based or donor_idx in skip_zero_based:\n",
    "        logger.info('\u00c3\u0153berspringe Paar target=%d donor=%d (Skip-Liste)', target_idx, donor_idx)\n",
    "        continue\n",
    "\n",
    "    roi_before, _ = extract_roi_patch(stack_work[target_idx], **ROI_CFG)\n",
    "    coeff, stats = estimate_coeff_roi(\n",
    "        stack_work[target_idx],\n",
    "        stack_work[donor_idx],\n",
    "        roi_cfg=ROI_CFG,\n",
    "        pad=ROI_PAD,\n",
    "    )\n",
    "    apply_spillover(stack_work, donor_idx, target_idx, coeff)\n",
    "    roi_after, _ = extract_roi_patch(stack_work[target_idx], **ROI_CFG)\n",
    "\n",
    "    target_info = channel_info.get(target_idx, {})\n",
    "    donor_info = channel_info.get(donor_idx, {})\n",
    "    target_marker = _clean_text(target_info.get('marker')) or (channel_names[target_idx] if target_idx < len(channel_names) else f'C{target_idx+1:02d}')\n",
    "    donor_marker = _clean_text(donor_info.get('marker')) or (channel_names[donor_idx] if donor_idx < len(channel_names) else f'C{donor_idx+1:02d}')\n",
    "    target_fluor = _clean_text(target_info.get('fluor'))\n",
    "    donor_fluor = _clean_text(donor_info.get('fluor'))\n",
    "\n",
    "    stats.update(\n",
    "        target_idx=int(target_idx),\n",
    "        donor_idx=int(donor_idx),\n",
    "        roi_sum_after=float(np.sum(roi_after)),\n",
    "        roi_mean_before=float(np.mean(roi_before)),\n",
    "        roi_mean_after=float(np.mean(roi_after)),\n",
    "        target_marker=target_marker,\n",
    "        target_fluor=target_fluor,\n",
    "        donor_marker=donor_marker,\n",
    "        donor_fluor=donor_fluor,\n",
    "    )\n",
    "    spill_results.append(stats)\n",
    "\n",
    "    ratio_cap_disp = stats.get('coeff_cap_ratio')\n",
    "    if ratio_cap_disp is None or ratio_cap_disp < 0:\n",
    "        ratio_cap_disp = float('inf')\n",
    "    target_desc = target_marker if not target_fluor else f\"{target_marker} [{target_fluor}]\"\n",
    "    donor_desc = donor_marker if not donor_fluor else f\"{donor_marker} [{donor_fluor}]\"\n",
    "    logger.info(\n",
    "        'C%d (%s) <- C%d (%s) | coeff=%.4f (raw %.4f, cap %.4f) | ROI-mean %.3f\u00e2\u2020\u2019%.3f',\n",
    "        target_idx + 1,\n",
    "        target_desc,\n",
    "        donor_idx + 1,\n",
    "        donor_desc,\n",
    "        stats['coeff'],\n",
    "        stats['coeff_raw'],\n",
    "        ratio_cap_disp,\n",
    "        stats['roi_mean_before'],\n",
    "        stats['roi_mean_after'],\n",
    "    )\n",
    "\n",
    "logger.info('Spillover-Korrektur abgeschlossen. Konvertiere zur\u00c3\u00bcck in %s.', orig_dtype)\n",
    "\n",
    "data_out = convert_to_dtype(stack_work, orig_dtype)\n",
    "spillover_dir, ome_path, channel_dir, spill_path = build_output_base(INPUT_PATH)\n",
    "channel_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata_labels = channel_labels if channel_labels else channel_names\n",
    "metadata = {'axes': 'CYX', 'Channel': {'Name': metadata_labels}}\n",
    "tiff.imwrite(str(ome_path), data_out, photometric='minisblack', metadata=metadata)\n",
    "logger.info('OME-TIFF gespeichert: %s', ome_path)\n",
    "\n",
    "for ci in range(data_out.shape[0]):\n",
    "    info = channel_info.get(ci, {})\n",
    "    marker_name = _clean_text(info.get('marker')) or (channel_names[ci] if ci < len(channel_names) else f'C{ci+1:02d}')\n",
    "    fluor_name = _clean_text(info.get('fluor'))\n",
    "    parts = [p for p in (marker_name, fluor_name) if p]\n",
    "    if not parts:\n",
    "        parts = [channel_labels[ci]]\n",
    "    label_core = '_'.join(_safe_token(part) for part in parts)\n",
    "    label = f\"C{ci+1:03d}_{label_core}\" if label_core else f\"C{ci+1:03d}\"\n",
    "    tiff.imwrite(str(channel_dir / f\"{label}.tif\"), data_out[ci], photometric='minisblack')\n",
    "logger.info('Kan\u00c3\u00a4le exportiert: %s', channel_dir)\n",
    "\n",
    "with open(spill_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(\n",
    "        dict(\n",
    "            pairs=spill_results,\n",
    "            params=dict(\n",
    "                roi=ROI_CFG,\n",
    "                pad=ROI_PAD,\n",
    "                percentile_q=PERCENTILE_Q,\n",
    "                safety_factor=SAFETY_FACTOR,\n",
    "                max_coeff=MAX_COEFF,\n",
    "            ),\n",
    "        ),\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "logger.info('Spillover-Log gespeichert: %s', spill_path)\n",
    "\n",
    "logger.info('Workflow fertig.')\n",
    "logger.info('  OME  : %s', ome_path)\n",
    "logger.info('  ChDir: %s', channel_dir)\n",
    "logger.info('  Spill: %s', spill_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055af377",
   "metadata": {},
   "source": [
    "## 5. Ergebnis-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OME-TIFF:', ome_path)\n",
    "print('Channel-Verzeichnis:', channel_dir)\n",
    "print('Spillover-Log:', spill_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Epoxy_CyNif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}